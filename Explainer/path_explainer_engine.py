# path_explainer_engine.py
#
# è¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„ "æ¨¡å‹éªŒè¯" å¼•æ“ã€‚
# å®ƒå¯ä»¥åˆ†æ UNet, GBM, GARCH ç­‰ä»»ä½•æ¨¡å‹çš„è¾“å‡ºã€‚

import sys
from pathlib import Path
import os
import numpy as np
import pandas as pd
import joblib
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
import ast

# --- å¯¼å…¥è‡ªå®šä¹‰æ¨¡å— ---
import Explainer.path_explainer_library as lib # æˆ‘ä»¬çš„ "å‡½æ•°åº“"

# --- è·¯å¾„å¯¼å…¥ ---
try:
    current_file_dir = Path(__file__).parent.resolve()
    project_root = current_file_dir.parent
    sys.path.append(str(project_root))
    import Project_Path as pp
    from Data.Input_preparation import DataProcessor # ç¡®ä¿èƒ½å¯¼å…¥
except (ImportError, NameError) as e:
    print(f"âŒ ä¸¥é‡é”™è¯¯: æœªèƒ½å¯¼å…¥ Project_Path æˆ– DataProcessor: {e}")
    sys.exit(1)

class PathExplainerEngine:
    """
    ä¸€ä¸ªé€šç”¨çš„ã€ç”±é…ç½®é©±åŠ¨çš„è·¯å¾„éªŒè¯å¼•æ“ã€‚
    å®ƒæ•´åˆäº† 5.1 å’Œ 5.2 çš„åŠŸèƒ½ã€‚
    """
    def __init__(self, asset_name, job_spec):
        self.asset_name = asset_name
        self.spec = job_spec
        self.job_name_safe = self.spec['display_name'].replace(' ', '_').replace('(', '').replace(')', '')
        self.model_type = self.spec['model_type']
        
        print(f"ğŸš€ æ­£åœ¨åˆå§‹åŒ–è·¯å¾„éªŒè¯å¼•æ“ (æ¨¡å‹: {self.spec['display_name']})...")
        
        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self._setup_paths()
        
        # å¼•æ“ç»„ä»¶
        self.data_processor = None
        self.generated_paths = None
        self.val_df = None
        
        plt.style.use('seaborn-v0_8-darkgrid')
        sns.set_palette("colorblind")

    def _setup_paths(self):
        """åˆ›å»ºæ‰€æœ‰è¾“å‡ºç›®å½•"""
        report_name = f"{self.timestamp}_{self.job_name_safe}_validation_report"
        self.report_dir = getattr(pp, "Report_Results_DIR") /"Path_Generator_Report"/ self.asset_name / self.job_name_safe / report_name
        self.images_dir = self.report_dir / "images"
        
        os.makedirs(self.report_dir, exist_ok=True)
        os.makedirs(self.images_dir, exist_ok=True)
        print(f"ğŸ“‚ æŠ¥å‘Šå°†ä¿å­˜è‡³: {self.report_dir}")
        print(f"ğŸ“Š å›¾è¡¨å°†ä¿å­˜è‡³: {self.images_dir}")

    def load_data(self):
        """
        æ ¹æ®ä½œä¸šè§„èŒƒ (job_spec) åŠ è½½æ‰€æœ‰å¿…éœ€çš„æ•°æ®ã€‚
        (å·²æ›´æ–°ï¼Œæ”¯æŒ data_asset_folder å’Œè·¯å¾„æ–‡ä»¶è‡ªåŠ¨æ£€æµ‹)
        """
        print("\n--- æ­¥éª¤1: åŠ è½½åˆ†ææ‰€éœ€æ–‡ä»¶ ---")

        # ç¡®å®šæ•°æ®æ–‡ä»¶çš„çœŸå®å­˜æ”¾ä½ç½®
        data_folder = self.spec.get('data_asset_folder', self.asset_name)
        if data_folder != self.asset_name:
            print(f"â„¹ï¸  æ­£åœ¨ä» '{data_folder}' æ–‡ä»¶å¤¹åŠ è½½æ•°æ®ï¼Œç”¨äºåˆ†æ '{self.asset_name}' èµ„äº§ã€‚")

        # 1. è‡ªåŠ¨æ£€æµ‹å¹¶åŠ è½½è·¯å¾„æ–‡ä»¶
        paths_dir_key = self.spec['paths_dir_key']
        paths_dir = getattr(pp, paths_dir_key, None)
        if paths_dir is None:
             raise AttributeError(f"Project_Path.py ç¼ºå°‘ '{paths_dir_key}' å˜é‡")

        target_dir = paths_dir / data_folder
        if not target_dir.exists():
            raise FileNotFoundError(f"æ•°æ®ç›®å½•æœªæ‰¾åˆ°: {target_dir}")

        # ä»é…ç½®ä¸­è·å–åŸºç¡€å
        base_name = self.spec.get('paths_filename_base')
        if not base_name:
            raise ValueError(f"ä½œä¸š '{self.job_name_safe}' çš„é…ç½®ä¸­ç¼ºå°‘ 'paths_filename_base' é”®")

        # å®šä¹‰æœç´¢æ¨¡å¼
        pattern = f"{base_name}_*_samples.npy"
        print(f"â„¹ï¸  æ­£åœ¨ '{target_dir}' ä¸­è‡ªåŠ¨æœç´¢æœ€æ–°è·¯å¾„æ–‡ä»¶ï¼Œæ¨¡å¼: '{pattern}'...")

        # æœç´¢æ‰€æœ‰åŒ¹é…æ–‡ä»¶ï¼Œå¹¶æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨æœ€å‰é¢ï¼‰
        try:
            matching_files = sorted(
                target_dir.glob(pattern), 
                key=os.path.getmtime, 
                reverse=True
            )
        except Exception as e:
            raise IOError(f"æœç´¢æ–‡ä»¶æ—¶å‡ºé”™: {e}")

        if not matching_files:
            raise FileNotFoundError(f"è‡ªåŠ¨æ£€æµ‹å¤±è´¥ï¼šåœ¨ '{target_dir}' ä¸­æœªæ‰¾åˆ°åŒ¹é… '{pattern}' çš„è·¯å¾„æ–‡ä»¶ã€‚")

        paths_file = matching_files[0] # è·å–æœ€æ–°çš„é‚£ä¸ªæ–‡ä»¶

        if len(matching_files) > 1:
            print(f"   âš ï¸ è­¦å‘Š: æ‰¾åˆ° {len(matching_files)} ä¸ªåŒ¹é…æ–‡ä»¶ã€‚å°†è‡ªåŠ¨ä½¿ç”¨æœ€æ–°çš„ä¸€ä¸ª: {paths_file.name}")

        self.generated_paths = np.load(paths_file)
        print(f"âœ… å·²åŠ è½½è·¯å¾„: {paths_file} (å½¢çŠ¶: {self.generated_paths.shape})")

        # --- 2. åŠ è½½éªŒè¯é›† (å·²ä¿®æ”¹ä¸ºä»ä¸­å¤® merge æ–‡ä»¶è¿‡æ»¤) ---
        print("   ğŸ”„ æ­£åœ¨åŠ è½½ *ä¸­å¤®* éªŒè¯æ•°æ® (testing_data_merged.csv) ä½œä¸ºåŸºå‡†...")
        
        # å‡è®¾åŸºå‡†æ•°æ®éƒ½åœ¨ Testing_DATA_DIR
        val_dir_key = "Testing_DATA_DIR" 
        val_base_dir = getattr(pp, val_dir_key, None)
        if val_base_dir is None: raise AttributeError(f"Project_Path.py ç¼ºå°‘ '{val_dir_key}'")

        # å‡è®¾ä¸­å¤®æ–‡ä»¶å (å¦‚æœä½ çš„æ–‡ä»¶åä¸åŒï¼Œè¯·åœ¨æ­¤å¤„ä¿®æ”¹)
        central_file_name = 'testing_data_merged.csv'
        val_file_path = val_base_dir / central_file_name
        
        if not val_file_path.exists():
            # å¤‡ç”¨ï¼šä¸‡ä¸€æ–‡ä»¶åæ˜¯ val_df.csv
            val_file_path_fallback = val_base_dir / 'val_df.csv'
            if val_file_path_fallback.exists():
                val_file_path = val_file_path_fallback
            else:
                 raise FileNotFoundError(f"ä¸­å¤® merge æ–‡ä»¶æœªæ‰¾åˆ°: {val_file_path} æˆ– {val_file_path_fallback}")

        try:
            full_val_df = pd.read_csv(val_file_path)
            print(f"   âœ… ä¸­å¤®éªŒè¯æ–‡ä»¶åŠ è½½æˆåŠŸ: {val_file_path}")

            # å…³é”®: *è¿‡æ»¤* DataFrame ä»¥è·å–å½“å‰èµ„äº§çš„å­é›†
            asset_to_filter = self.asset_name # e.g., 'CSI1000'
            
            # !! å…³é”®å‡è®¾ !! 
            # å‡è®¾ç”¨äºè¿‡æ»¤çš„åˆ—åæ˜¯ 'asset_name'
            # å¦‚æœä½ çš„åˆ—åä¸åŒ (ä¾‹å¦‚ 'index')ï¼Œè¯·åœ¨æ­¤å¤„ä¿®æ”¹
            filter_column_name = 'asset_underlying' 
            
            print(f"   ğŸ”„ æ­£åœ¨è¿‡æ»¤ '{asset_to_filter}' çš„å­é›† (åŸºäºåˆ— '{filter_column_name}')...")
            if filter_column_name not in full_val_df.columns:
                 raise KeyError(f"ä¸­å¤® merge æ–‡ä»¶ '{val_file_path}' ä¸­ç¼ºå°‘ç”¨äºè¿‡æ»¤çš„åˆ—: '{filter_column_name}'")

            self.val_df = full_val_df[full_val_df[filter_column_name] == asset_to_filter].copy()

            if self.val_df.empty:
                raise ValueError(f"åœ¨ '{val_file_path}' ä¸­æ‰¾ä¸åˆ°èµ„äº§ '{asset_to_filter}' çš„ä»»ä½•æ•°æ®ã€‚")
            
            print(f"âœ… éªŒè¯å­é›†åŠ è½½æˆåŠŸã€‚å°†ä½¿ç”¨ {len(self.val_df)} ä¸ª '{asset_to_filter}' å¸‚åœºæ¡ä»¶ã€‚")
        
        except Exception as e:
            print(f"âŒ åŠ è½½æˆ–å¤„ç†ä¸­å¤® merge æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            raise

        # 3. (æ¡ä»¶) åŠ è½½ DataProcessor (é€»è¾‘æ›´æ–°ä»¥åŒ¹é…è·¯å¾„)
        if self.model_type == 'unet':
            proc_dir_key = self.spec['processor_dir_key']
            proc_dir = getattr(pp, proc_dir_key, None)
            if proc_dir is None:
                raise AttributeError(f"Project_Path.py ç¼ºå°‘ '{proc_dir_key}' å˜é‡")

            processor_folder = self.spec.get('processor_source_folder', data_folder)
            processor_type_subfolder = self.spec.get('processor_type_subfolder', None)
            if processor_type_subfolder:
                proc_file_path = proc_dir / processor_type_subfolder / processor_folder / self.spec['processor_filename']
            else:
                proc_file_path = proc_dir / processor_folder / self.spec['processor_filename']

            if not proc_file_path.exists():
                raise FileNotFoundError(f"DataProcessor æœªæ‰¾åˆ°: {proc_file_path}")

            self.data_processor = joblib.load(proc_file_path)
            print(f"âœ… å·²åŠ è½½ DataProcessor: {proc_file_path}")
        else:
            print("â„¹ï¸  MC æ¨¡å‹åˆ†ææ— éœ€åŠ è½½ DataProcessorã€‚")

    def run_analysis(self, indices_to_analyze):
        """
        å¯¹é€‰å®šçš„ç´¢å¼•æ‰§è¡Œå¾ªç¯åˆ†æã€‚
        """
        print(f"\n--- æ­¥éª¤2: å¼€å§‹åˆ†æ {len(indices_to_analyze)} ä¸ªå¸‚åœºæ¡ä»¶ ---")
        all_results = []
        
        num_conditions = min(len(indices_to_analyze), self.generated_paths.shape[0], len(self.val_df))
        
        for idx in tqdm(indices_to_analyze, desc="åˆ†æè¿›åº¦", total=num_conditions):
            if idx >= num_conditions:
                print(f"è­¦å‘Š: ç´¢å¼• {idx} è¶…å‡ºæ•°æ®èŒƒå›´ï¼Œåœæ­¢ã€‚")
                break
                
            try:
                result = self._analyze_single_condition(idx)
                all_results.append(result)
            except Exception as e:
                print(f"âŒ æ¡ä»¶ {idx} åˆ†æå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        print(f"\nâœ… å·²å®Œæˆ {len(all_results)} ä¸ªæ¡ä»¶çš„åˆ†æã€‚")
        return all_results

    def _analyze_single_condition(self, idx):
        """
        å¯¹å•ä¸ªæ¡ä»¶æ‰§è¡Œå…¨å¥—åˆ†æ (è°ƒç”¨å‡½æ•°åº“)ã€‚
        è¿™æ˜¯ UNet (5.1) å’Œ MC (5.2) é€»è¾‘çš„èåˆç‚¹ã€‚
        """
        condition_info = self.val_df.iloc[idx]
        real_price_path = np.array(ast.literal_eval(condition_info['price_series']))
        all_restored_prices = None # æœ€ç»ˆéœ€è¦çš„å½¢çŠ¶: (N_sim, SeqLen)
        
        if self.model_type == 'unet':
            # UNet è¾“å‡ºæ˜¯å¯¹æ•°æ”¶ç›Šç‡ï¼Œå½¢çŠ¶: (N_sim, Channels=1, SeqLen)
            ensemble_log_returns = self.generated_paths[idx]
            # (ç¡®ä¿ recover å‡½æ•°è¿”å›çš„æ˜¯ (N_sim, SeqLen) æˆ–è¿›è¡Œç›¸åº”è°ƒæ•´)
            all_restored_prices = lib.recover_price_paths_from_returns(
                ensemble_log_returns, 
                condition_info['start_price'], 
                self.data_processor
            )
            # å¦‚æœ recover å‡½æ•°è¿”å› (N_sim, 1, SeqLen)ï¼Œä¹Ÿéœ€è¦ squeeze
            if all_restored_prices.ndim == 3 and all_restored_prices.shape[1] == 1:
                all_restored_prices = all_restored_prices.squeeze(axis=1)

        else: # 'mc' (GBM/GARCH)
            # MC ä¿å­˜çš„æ–‡ä»¶å½¢çŠ¶æ˜¯ (N_cond * N_sim, 1, SeqLen)
            
            # --- !! ä¿®æ”¹è¿™é‡Œçš„é€»è¾‘ !! ---
            # 1. è®¡ç®—æ¯ä¸ªæ¡ä»¶æœ‰å¤šå°‘ä¸ª simulation (N_sim)
            #    (å‡è®¾æ‰€æœ‰æ¡ä»¶éƒ½æœ‰ç›¸åŒæ•°é‡çš„ simulation)
            num_conditions_in_file = len(self.val_df) # æˆ–è€…ä»æ–‡ä»¶åè§£æï¼Ÿæ›´å®‰å…¨çš„æ˜¯ç”¨ val_df é•¿åº¦
            if self.generated_paths.shape[0] % num_conditions_in_file != 0:
                raise ValueError("åŠ è½½çš„ MC è·¯å¾„æ–‡ä»¶æ€»è¡Œæ•°æ— æ³•è¢«æ¡ä»¶æ•°é‡æ•´é™¤ï¼Œå½¢çŠ¶å¯èƒ½ä¸åŒ¹é…ã€‚")
            n_sim = self.generated_paths.shape[0] // num_conditions_in_file
            
            # 2. è®¡ç®—å½“å‰æ¡ä»¶ idx å¯¹åº”çš„åˆ‡ç‰‡èŒƒå›´
            start_row = idx * n_sim
            end_row = (idx + 1) * n_sim
            
            # 3. æå–å±äºè¯¥æ¡ä»¶çš„æ‰€æœ‰ simulationï¼Œå½¢çŠ¶ (N_sim, 1, SeqLen)
            ensemble_prices_for_idx = self.generated_paths[start_row:end_row]
            
            # 4. ç°åœ¨ squeeze(axis=1) å¯ä»¥æ­£å¸¸å·¥ä½œäº†ï¼Œå¾—åˆ° (N_sim, SeqLen)
            all_restored_prices = ensemble_prices_for_idx.squeeze(axis=1)
            # --- !! ä¿®æ”¹ç»“æŸ !! ---

        if all_restored_prices is None or all_restored_prices.ndim != 2:
             raise ValueError(f"æ¡ä»¶ {idx}: æœªèƒ½æ­£ç¡®å‡†å¤‡ä»·æ ¼è·¯å¾„ï¼Œæœ€ç»ˆå½¢çŠ¶ä¸º {all_restored_prices.shape if all_restored_prices is not None else 'None'}")
        
        # --- 2. é€šç”¨æ­¥éª¤ï¼šè®¡ç®—æ”¶ç›Šç‡ ---
        valid_length = len(real_price_path)
        real_log_returns = np.diff(np.log(real_price_path))
        
        valid_prices = all_restored_prices[:, :valid_length]
        valid_prices = valid_prices[~np.isnan(valid_prices).any(axis=1)] # ç§»é™¤ GARCH çš„ NaN
        if len(valid_prices) == 0:
            raise ValueError(f"æ¡ä»¶ {idx} æ²¡æœ‰æœ‰æ•ˆçš„ç”Ÿæˆè·¯å¾„ï¼ˆå¯èƒ½å…¨ä¸ºNaNï¼‰ã€‚")
            
        generated_returns = np.diff(np.log(valid_prices), axis=1)
        
        # --- 3. é€šç”¨æ­¥éª¤ï¼šè°ƒç”¨å‡½æ•°åº“ (ä½¿ç”¨ 5.1 çš„é«˜çº§æ ‡å‡†) ---
        
        # ä½¿ç”¨ 5.1 çš„é«˜çº§ç»Ÿè®¡
        stats_results = lib.calculate_comprehensive_statistics(real_log_returns, generated_returns)
        
        # ç»˜å›¾
        plot_paths = {}
        model_name = self.spec['display_name']
        
        plot_paths['fan_chart'] = self.images_dir / f"fan_chart_cond_{idx}.png"
        lib.plot_enhanced_fan_chart(valid_prices, real_price_path, condition_info, plot_paths['fan_chart'], model_name)
        
        plot_paths['qq_plot'] = self.images_dir / f"qq_plot_cond_{idx}.png"
        qq_r_squared = lib.plot_qq_comparison(real_log_returns, generated_returns, condition_info, plot_paths['qq_plot'], model_name)
        stats_results['qq_r_squared'] = qq_r_squared

        # ä½¿ç”¨ 5.2 çš„è¡¥å……å›¾
        plot_paths['distribution'] = self.images_dir / f"dist_plot_cond_{idx}.png"
        lib.plot_return_distribution(real_log_returns, generated_returns, condition_info, plot_paths['distribution'], model_name)
        
        plot_paths['vol_clustering'] = self.images_dir / f"vol_cluster_cond_{idx}.png"
        lib.plot_volatility_clustering(real_log_returns, generated_returns, condition_info, plot_paths['vol_clustering'], model_name)

        return {
            'condition_idx': idx,
            'condition_info': condition_info.to_dict(),
            'statistics': stats_results,
            'plots': {k: str(v) for k, v in plot_paths.items()} # å­˜å‚¨ä¸ºå­—ç¬¦ä¸²è·¯å¾„
        }

    def generate_report(self, all_results):
        """
        è®¡ç®—æœ€ç»ˆè¯„åˆ†å¹¶ä¿å­˜æ‰€æœ‰æŠ¥å‘Š (CSV, Markdown)ã€‚
        (é€»è¾‘æ¥è‡ª 5.1)
        """
        if not all_results:
            print("âŒ æ²¡æœ‰åˆ†æç»“æœï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Šã€‚")
            return
            
        print(f"\n--- æ­¥éª¤3: è®¡ç®—æ¨¡å‹ç»¼åˆè¯„åˆ† ---")
        # ä½¿ç”¨ 5.1 çš„é«˜çº§è¯„åˆ†ç³»ç»Ÿ
        model_scores = lib.calculate_model_score(all_results)
        
        print(f"   æ¨¡å‹ç»¼åˆè¯„åˆ†: {model_scores['overall_score']:.2f}/100")
        print(f"   è¯„åˆ†æ ‡å‡†å·®: {model_scores['score_std']:.2f}")
        print(f"   æ¨¡å‹ç­‰çº§: {model_scores['grade']}")

        print(f"\n--- æ­¥éª¤4: ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š ---")
        self._save_results_to_csv(all_results, model_scores)
    
    def _save_results_to_csv(self, all_results, model_scores):
        """ç§æœ‰è¾…åŠ©å‡½æ•°ï¼šä¿å­˜ CSV (æ¥è‡ª 5.1)"""
        
        detailed_stats = [
            {'condition_idx': r['condition_idx'], **r['statistics']} 
            for r in all_results
        ]
        detailed_df = pd.DataFrame(detailed_stats)
        detailed_path = self.report_dir / 'detailed_statistics.csv'
        detailed_df.to_csv(detailed_path, index=False, encoding='utf-8-sig')
        
        # æ±‡æ€» (æ¥è‡ª 5.1)
        summary_df = pd.DataFrame(columns=['metric', 'mean', 'std', 'min', 'max'])
        metrics_mapping = {
            'Mean Difference': 'mean_diff', 'Volatility Difference': 'vol_diff',
            'Skewness Difference': 'skew_diff', 'Kurtosis Difference': 'kurt_diff',
            'AD Statistic': 'ad_statistic', 'AD Rejection Level (%)': 'ad_rejection_level',
            'KS Statistic': 'ks_statistic', 'KS P-value': 'ks_pvalue',
            'Wasserstein Distance': 'wasserstein_distance', 'QQ R-squared': 'qq_r_squared',
            'VaR 1% Difference': 'var_1_diff', 'VaR 99% Difference': 'var_99_diff',
            'CVaR 1% Difference': 'cvar_1_diff'
        }
        
        summary_data = []
        for metric_name, col_name in metrics_mapping.items():
            if col_name in detailed_df.columns:
                values = detailed_df[col_name].dropna()
                if col_name == 'ad_rejection_level': values *= 100
                summary_data.append({
                    'metric': metric_name,
                    'mean': values.mean(), 'std': values.std(),
                    'min': values.min(), 'max': values.max()
                })
        
        summary_df = pd.DataFrame(summary_data)
        summary_path = self.report_dir / 'summary_statistics.csv'
        summary_df.to_csv(summary_path, index=False, encoding='utf-8-sig')
        
        # è¯„åˆ† (æ¥è‡ª 5.1)
        score_report = {
            'overall_score': [model_scores['overall_score']],
            'score_std': [model_scores['score_std']],
            'grade': [model_scores['grade']],
            'total_conditions_analyzed': [len(all_results)],
            'analysis_date': [datetime.now().strftime('%Y-%m-%d %H:%M:%S')],
            'model_type': [self.spec['display_name']],
            'underlying_asset': [self.asset_name]
        }
        score_df = pd.DataFrame(score_report)
        score_path = self.report_dir / 'model_evaluation_report.csv'
        score_df.to_csv(score_path, index=False, encoding='utf-8-sig')
        
        print(f"   âœ… CSV æŠ¥å‘Šå·²ä¿å­˜è‡³: {self.report_dir}")
        self._generate_markdown_report(all_results, summary_df, model_scores)

    def _generate_markdown_report(self, all_results, summary_df, model_scores):
        """ç§æœ‰è¾…åŠ©å‡½æ•°ï¼šç”Ÿæˆ Markdown æŠ¥å‘Š (æ¥è‡ª 5.1 & 5.2)"""
        report_path = self.report_dir / 'Model_Validation_Report.md'
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"# æ¨¡å‹éªŒè¯æŠ¥å‘Š: {self.spec['display_name']}\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**æ ‡çš„èµ„äº§**: {self.asset_name}\n\n")
            
            f.write(f"## 1. æ¨¡å‹ç»¼åˆè¯„åˆ†\n\n")
            f.write(f"| æŒ‡æ ‡ | ç»“æœ |\n")
            f.write(f"| :--- | :--- |\n")
            f.write(f"| **æ€»åˆ† (Overall Score)** | **{model_scores['overall_score']:.2f} / 100** |\n")
            f.write(f"| **è¯„çº§ (Grade)** | **{model_scores['grade']}** |\n")
            f.write(f"| è¯„åˆ†æ ‡å‡†å·® (Score Std.) | {model_scores['score_std']:.2f} |\n")
            f.write(f"| åˆ†æçš„æ¡ä»¶æ€»æ•° | {len(all_results)} |\n\n")

            f.write(f"## 2. æ ¸å¿ƒæŒ‡æ ‡æ±‡æ€» (æ‰€æœ‰æ¡ä»¶å¹³å‡)\n\n")
            f.write(summary_df.to_markdown(index=False, floatfmt=".4f"))
            
            f.write(f"\n\n## 3. åˆ†åœºæ™¯è¯¦ç»†åˆ†æ (æŠ½æ ·)\n\n")
            f.write(f"*ä»…æ˜¾ç¤ºå‰ 10 ä¸ªåˆ†æçš„åœºæ™¯ã€‚*\n\n")

            for result in all_results[:10]: # æœ€å¤šæ˜¾ç¤ºå‰10ä¸ª
                idx = result['condition_idx']
                stats_df = pd.DataFrame([result['statistics']]).T.reset_index()
                stats_df.columns = ['Metric', 'Value']
                
                f.write(f"\n---\n\n### åœºæ™¯ (Condition) {idx}\n\n")
                condition_df = pd.Series(result['condition_info']).to_frame().T
                f.write(f"**åˆå§‹æ¡ä»¶:**\n")
                f.write(condition_df[['start_price', 'volatility', 'risk_free_rate', 'actual_trading_days']].to_markdown(index=False))
                f.write(f"\n\n**ç»Ÿè®¡æŒ‡çº¹ (Statistics):**\n")
                f.write(stats_df.to_markdown(index=False, floatfmt=".4f"))
                f.write(f"\n\n**å›¾è¡¨:**\n\n")
                
                # åŠ¨æ€ç”Ÿæˆç›¸å¯¹è·¯å¾„
                img_path = Path(result['plots']['fan_chart']).relative_to(self.report_dir)
                f.write(f"![Fan Chart {idx}]({img_path.as_posix()})\n")
                img_path = Path(result['plots']['qq_plot']).relative_to(self.report_dir)
                f.write(f"![QQ Plot {idx}]({img_path.as_posix()})\n")
                img_path = Path(result['plots']['distribution']).relative_to(self.report_dir)
                f.write(f"![Distribution {idx}]({img_path.as_posix()})\n")
                img_path = Path(result['plots']['vol_clustering']).relative_to(self.report_dir)
                f.write(f"![Volatility Clustering {idx}]({img_path.as_posix()})\n")

        print(f"   âœ… Markdown æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
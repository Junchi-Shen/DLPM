# -*- coding: utf-8 -*-
import sys
from pathlib import Path
import os
import time
from datetime import datetime
import pandas as pd
import numpy as np
import torch
import joblib
import traceback
import json
import matplotlib.pyplot as plt

# --- 1. è·¯å¾„è®¾ç½®ä¸ sys.path æ³¨å…¥ ---
current_file_dir = Path(__file__).parent.resolve()
project_root = current_file_dir.parent
sys.path.append(str(project_root))

# è‡ªåŠ¨å®šä½å¹¶æ³¨å…¥æ¨¡å‹ç›®å½•
diffusion_model_dir = project_root / 'Model' / 'Diffusion_Model_DLPM'
if diffusion_model_dir.exists():
    sys.path.insert(0, str(diffusion_model_dir))
    print(f"[è·¯å¾„ä¿®æ­£] å·²æ³¨å…¥æ¨¡å‹ç›®å½•: {diffusion_model_dir}")

# --- 2. å¯¼å…¥é¡¹ç›®æ¨¡å— ---
try:
    import Project_Path as pp
    from Data.Input_preparation import DataProcessor
    from Model.Diffusion_Model.diffusion_with_condition import GaussianDiffusion1D
    from Model.Diffusion_Model.diffusion_dlpm import DLPMDiffusion1D
    from Model.Diffusion_Model.trainer_with_condition import Trainer1D, Dataset1D
    from Model.Diffusion_Model.Unet_with_condition import Unet1D
    from Model.Diffusion_Model.condition_network import EnhancedConditionNetwork
    import Config.Diffusion_config_DLPM as DiffusionDLPMConfig
except ImportError as e:
    print(f"âŒ é”™è¯¯ï¼šå¯¼å…¥é¡¹ç›®æ¨¡å—å¤±è´¥: {e}")
    sys.exit(1)

# --- 3. æŠ¥å‘Šç”Ÿæˆå‡½æ•° ---
def generate_training_report(report_path: Path, config: dict, data_info: dict, model_info: dict, training_results: dict, artifact_paths: dict):
    """ç”Ÿæˆ Markdown è®­ç»ƒæ€»ç»“æŠ¥å‘Š"""
    try:
        report_path.parent.mkdir(parents=True, exist_ok=True)
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"# æ‰©æ•£æ¨¡å‹è®­ç»ƒæŠ¥å‘Š (å¤šç»´åº¦è‡ªé€‚åº”ç‰ˆ)\n\n")
            f.write(f"- **ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"- **è®­ç»ƒæ¨¡å¼**: {config.get('underlying_asset', 'N/A')} (å›½å®¶è¿‡æ»¤: {config.get('country', 'all')})\n\n")
            
            f.write("## æ•°æ®ç»Ÿè®¡\n")
            f.write(f"- æœ€ç»ˆæ ·æœ¬æ•°: {data_info.get('num_samples', 0):,}\n")
            f.write(f"- æ£€æµ‹åˆ°å›½å®¶/æŒ‡æ•°: {data_info.get('num_countries')}/{data_info.get('num_indices')}\n\n")

            f.write("## è®­ç»ƒç»“æœ\n")
            f.write(f"- æœ€ç»ˆ Loss: {training_results.get('final_loss', 'N/A'):.6f}\n")
            f.write(f"- è®­ç»ƒè€—æ—¶: {training_results.get('duration_seconds', 0)/60:.2f} åˆ†é’Ÿ\n\n")
            
            if artifact_paths.get('loss_curve'):
                f.write(f"![æŸå¤±æ›²çº¿](./{Path(artifact_paths['loss_curve']).name})\n")
    except Exception as e:
        print(f"âš ï¸ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")

# --- 4. ä¸»æ‰§è¡Œæµç¨‹ ---
if __name__ == '__main__':
    training_start_time = time.time()
    timestamp_str = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # [1] é…ç½®åŠ è½½
    main_config = DiffusionDLPMConfig.main_config
    print(f"The alpha of this model is {main_config['dlpm_alpha']}") 
    asset_name = main_config["underlying_asset"]     # 'all' æˆ– 'CSI1000'
    target_countries = main_config.get("country", "all") # 'CN', 'US', 'all' æˆ–åˆ—è¡¨
    if isinstance(target_countries, str): target_countries = [target_countries]

    print(f"\n" + "="*50)
    print(f"ğŸ DLPMæ‰©æ•£æ¨¡å‹è®­ç»ƒå¯åŠ¨å™¨")
    print(f"èµ„äº§æ¨¡å¼: {asset_name} | å›½å®¶é™åˆ¶: {target_countries}")
    print("="*50)

    # [2] åŠ¨æ€è·¯ç”±ä¸æ•°æ®è¿‡æ»¤ (æ ¸å¿ƒè¯‰æ±‚å®ç°)
    print("\n--- æ­¥éª¤ 1: æ•°æ®é€»è¾‘è·¯ç”± ---")
    temp_csv_path = None
    try:
        # A. ç¡®å®šåŸºç¡€æº
        if asset_name.lower() == 'all':
            base_path = pp.Trainning_DATA_DIR / 'trainning_data_merged.csv'
            print(f"ğŸ“‚ è·¯ç”±ï¼šä½¿ç”¨ä¸­å¤®åˆå¹¶å¤§è¡¨")
        else:
            base_path = pp.Trainning_DATA_DIR / asset_name / 'train_df.csv'
            print(f"ğŸ“‚ è·¯ç”±ï¼šä½¿ç”¨ç‰¹å®šèµ„äº§ç›®å½• -> {asset_name}")

        if not base_path.exists():
            raise FileNotFoundError(f"æ— æ³•å®šä½æ•°æ®æº: {base_path}")

        # B. å†…å­˜è¿‡æ»¤é€»è¾‘
        df = pd.read_csv(base_path)
        # å…¼å®¹æ€§æ£€æµ‹ï¼šå›½å®¶åˆ—åå¯èƒ½ä¸º 'country' æˆ– 'country_code'
        c_col = next((c for c in ['country_code', 'country'] if c in df.columns), None)
        
        if c_col and "all" not in [x.lower() for x in target_countries]:
            df = df[df[c_col].isin(target_countries)]
            print(f"ğŸ¯ è¿‡æ»¤ï¼šå·²ä¿ç•™å›½å®¶ {target_countries}, å‰©ä½™è¡Œæ•°: {len(df)}")
        
        if df.empty:
            raise ValueError("âŒ è¿‡æ»¤åæ— æœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥é…ç½®ä¸­çš„ country å‚æ•°ã€‚")

        # C. ç”ŸæˆåŠ¨æ€ä¸´æ—¶æ–‡ä»¶ (è§£è€¦ DataProcessor)
        temp_csv_path = pp.Trainning_DATA_DIR / f"temp_run_{asset_name}_{timestamp_str}.csv"
        df.to_csv(temp_csv_path, index=False)
        
    except Exception as e:
        print(f"âŒ æ•°æ®è·¯ç”±å¤±è´¥: {e}"); sys.exit(1)

    # [3] ç‰¹å¾æå–
    print("\n--- æ­¥éª¤ 2: ç‰¹å¾æå–ä¸å½’ä¸€åŒ– ---")
    try:
        data_processor = DataProcessor(main_config)
        X_train, y_train, mask_train = data_processor.process_all_data(temp_csv_path)
        for name, tensor in [("æ¡ä»¶ç‰¹å¾(X)", X_train), ("ç›®æ ‡åºåˆ—(y)", y_train), ("æœ‰æ•ˆæ€§Mask", mask_train)]:
            if not torch.isfinite(tensor).all():
                num_nan = torch.isnan(tensor).sum().item()
                num_inf = torch.isinf(tensor).sum().item()
                print(f"âŒ æ•°æ®å¼‚å¸¸ï¼š{name} åŒ…å« {num_nan} ä¸ª NaN, {num_inf} ä¸ª Inf")
        
                # å®šä½å…·ä½“çš„æ ·æœ¬ ID (å‡è®¾ X_train ç¬¬ä¸€ç»´æ˜¯ batch)
                error_indices = torch.where(~torch.isfinite(tensor).any(dim=-1).any(dim=-1))[0]
                print(f"ğŸš¨ å‡ºé”™æ ·æœ¬ç´¢å¼•ï¼ˆå‰5ä¸ªï¼‰: {error_indices[:5].tolist()}")
                raise ValueError(f"æ•°æ®æº {name} å­˜åœ¨æ•°å€¼æ±¡æŸ“ï¼Œè¯·æ£€æŸ¥ DataProcessor é€»è¾‘ã€‚")

            print("âœ… æ•°æ®æºå…¨é‡æ£€æŸ¥é€šè¿‡ï¼šFinite check passed.")
        
        data_info = {
            'source_file': str(base_path),
            'num_samples': len(X_train),
            'condition_dim': X_train.shape[-1],
            'sequence_length': y_train.shape[-1],
            'num_countries': data_processor.num_countries,
            'num_indices': data_processor.num_indices
        }
    except Exception as e:
        print(f"âŒ æ•°æ®å¤„ç†å¤±è´¥: {e}"); traceback.print_exc(); sys.exit(1)

    # [4] æ¨¡å‹æ¶æ„ä¸ç‰©ç†å‚æ•°å¯¹é½
    print("\n--- æ­¥éª¤ 3: æ¶æ„åˆå§‹åŒ– (å¸¦å…¨å±€ ID å†—ä½™) ---")
    device = "cuda" if torch.cuda.is_available() else "cpu"

    print(f"The device is {device}")
    # æ ¸å¿ƒï¼šä¸ºäº†è®©ä¸åŒå›½å®¶çš„æ•°æ®é›†èƒ½å…±ç”¨æ¨¡å‹ï¼Œæˆ‘ä»¬ç»™ Embedding ç•™å‡ºå†—ä½™ç©ºé—´
    # é˜²æ­¢æœªæ¥åŠ è½½ 'all' æ¨¡å‹æ—¶å› ä¸ºç±»åˆ«æ•°å¢åŠ è€ŒæŠ¥ç»´åº¦é”™è¯¯
    cond_net_params = main_config.get('cond_net_params', {})
    condition_network = EnhancedConditionNetwork(
        num_countries=max(data_info['num_countries'] + 5, 20), # æœ€å°‘é¢„ç•™ 20 ä¸ªå›½å®¶ä½ç½®
        num_indices=max(data_info['num_indices'] + 10, 100),   # æœ€å°‘é¢„ç•™ 100 ä¸ªæŒ‡æ•°ä½ç½®
        **cond_net_params
    ).to(device)

    model = Unet1D(
        cond_dim=cond_net_params.get('output_dim', 128), 
        **main_config.get('unet_params', {})
    ).to(device)

    # æ‰©æ•£è¿‡ç¨‹é€‰æ‹©
    use_dlpm = main_config.get('use_dlpm', True)
    if use_dlpm:
        diffusion = DLPMDiffusion1D(
            model=model, 
            condition_network=condition_network,
            alpha=main_config.get('dlpm_alpha', 1.75),
            **main_config
        ).to(device)
    else:
        diffusion = GaussianDiffusion1D(
            model=model, 
            condition_network=condition_network,
            **main_config
        ).to(device)

    # [5] è®­ç»ƒå¾ªç¯
    print("\n--- æ­¥éª¤ 4: æ‰§è¡Œè®­ç»ƒ ---")
    
    trainer_params = main_config.get('trainer_params', {})
    dataset = Dataset1D(y_train, X_train, mask_train)
    trainer = Trainer1D(
        diffusion_model=diffusion, 
        dataset=dataset, 
        results_folder=str(pp.Model_Results_DIR / 'Diffusion_Model_DLPM' / asset_name / 'checkpoints'),
        # ä» main_config æˆ–å…¶å­é¡¹ trainer_params ä¸­æ˜¾å¼æå–å‚æ•°
        train_batch_size=trainer_params.get('train_batch_size', main_config.get('train_batch_size', 64)),
        train_lr=trainer_params.get('train_lr', main_config.get('train_lr', 1e-6)),
        train_num_steps=main_config.get('train_num_steps', 20000),
        gradient_accumulate_every=trainer_params.get('gradient_accumulate_every', 1),
        ema_decay=trainer_params.get('ema_decay', main_config.get('ema_decay', 0.995)),
        amp=trainer_params.get('amp', main_config.get('amp', True))
    )
    
    trainer.train()

    # [6] ä¿å­˜ä¸æ¸…ç†
    print("\n--- æ­¥éª¤ 5: äº§å‡ºç‰©æŒä¹…åŒ– ---")
    model_dir = pp.Model_Results_DIR / 'Diffusion_Model_DLPM' / asset_name
    model_dir.mkdir(parents=True, exist_ok=True)
    
    # æ–‡ä»¶åå®šä¹‰
    suffix = "_all" if asset_name.lower() == 'all' else f"_{asset_name}"
    paths = {
        'model': model_dir / f"unet_conditional_model{suffix}.pth",
        'cond_net': model_dir / f"condition_network{suffix}.pth",
        'processor': model_dir / f"data_processor{suffix}.pkl",
        'report': pp.Results_DIR / "training_report" / "Diffusion_Model_DLPM" / asset_name / f"report_{timestamp_str}.md",
        'loss_curve': pp.Results_DIR / "training_report" / "Diffusion_Model_DLPM" / asset_name / f"loss_{timestamp_str}.png"
    }

    torch.save(model.state_dict(), paths['model'])
    torch.save(condition_network.state_dict(), paths['cond_net'])
    joblib.dump(data_processor, paths['processor'])
    
    paths['loss_curve'].parent.mkdir(parents=True, exist_ok=True)
    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    if trainer.loss_history:
        plt.figure(figsize=(10, 5))
        plt.plot(trainer.loss_history)
        plt.title(f"Loss Curve - {asset_name}"); plt.yscale('log')
        plt.savefig(paths['loss_curve']); plt.close()

    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    if temp_csv_path and temp_csv_path.exists():
        temp_csv_path.unlink()
        print(f"ğŸ§¹ å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶")

    # ç”ŸæˆæŠ¥å‘Š
    training_results = {
        'steps_run': trainer.step,
        'final_loss': trainer.loss_history[-1] if trainer.loss_history else None,
        'duration_seconds': time.time() - training_start_time
    }
    generate_training_report(paths['report'], main_config, data_info, {'type': 'U-Net', 'unet_params': sum(p.numel() for p in model.parameters()), 'cond_net_params': sum(p.numel() for p in condition_network.parameters()), 'total_params': sum(p.numel() for p in model.parameters()) + sum(p.numel() for p in condition_network.parameters())}, training_results, {'model': str(paths['model']), 'condition_network': str(paths['cond_net']), 'processor': str(paths['processor']), 'loss_curve': paths['loss_curve']})

    print(f"\nâœ… èµ„äº§ '{asset_name}' çš„è®­ç»ƒä»»åŠ¡å·²åœ†æ»¡å®Œæˆï¼")
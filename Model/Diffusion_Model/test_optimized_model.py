#!/usr/bin/env python3
"""
æµ‹è¯•ä¼˜åŒ–çš„UNetæ¨¡å‹
å³ä½¿æ²¡æœ‰C++æ‰©å±•ä¹Ÿèƒ½è¿è¡Œ
"""

import torch
import sys
from pathlib import Path

def test_model():
    """æµ‹è¯•ä¼˜åŒ–æ¨¡å‹"""
    print("ğŸ§ª æµ‹è¯•UNetä¼˜åŒ–æ¨¡å‹")
    print("=" * 50)
    
    # æ£€æŸ¥è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸ è®¾å¤‡: {device}")
    
    try:
        # å¯¼å…¥ä¼˜åŒ–æ¨¡å‹
        from Unet_with_condition_optimized import Unet1D
        print("âœ… æˆåŠŸå¯¼å…¥ä¼˜åŒ–æ¨¡å‹")
        
        # æ£€æŸ¥C++æ‰©å±•çŠ¶æ€
        try:
            from cpp_extension.unet_cpp_wrapper import CPP_AVAILABLE
        except ImportError:
            CPP_AVAILABLE = False
        
        if CPP_AVAILABLE:
            print("âœ… C++åŠ é€Ÿæ‰©å±•: å¯ç”¨")
        else:
            print("âš ï¸ C++åŠ é€Ÿæ‰©å±•: ä¸å¯ç”¨ï¼ˆä½¿ç”¨PyTorchåŸç”Ÿå®ç°ï¼‰")
        
    except ImportError as e:
        print(f"âŒ æ— æ³•å¯¼å…¥ä¼˜åŒ–æ¨¡å‹: {e}")
        print("å›é€€åˆ°åŸå§‹æ¨¡å‹...")
        from Unet_with_condition import Unet1D
        CPP_AVAILABLE = False
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    print("\nğŸ“Š åˆ›å»ºæµ‹è¯•æ•°æ®...")
    batch_size = 2
    seq_length = 50
    channels = 3
    
    x = torch.randn(batch_size, channels, seq_length).to(device)
    time = torch.randint(0, 1000, (batch_size,)).to(device)
    cond_input = torch.randn(batch_size, 10).to(device)
    
    print(f"   è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"   æ—¶é—´æ­¥: {time.shape}")
    print(f"   æ¡ä»¶è¾“å…¥: {cond_input.shape}")
    
    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ”¨ åˆ›å»ºæ¨¡å‹...")
    model_config = {
        'dim': 32,
        'init_dim': 32,
        'dim_mults': (1, 2),
        'channels': channels,
        'cond_dim': 10
    }
    
    model = Unet1D(**model_config).to(device)
    model.eval()
    
    print(f"   æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    print("\nâš¡ æµ‹è¯•å‰å‘ä¼ æ’­...")
    try:
        with torch.no_grad():
            output = model(x, time, cond_input=cond_input)
        
        print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ")
        print(f"   è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"   è¾“å‡ºèŒƒå›´: [{output.min():.3f}, {output.max():.3f}]")
        
        # æ£€æŸ¥è¾“å‡ºæ˜¯å¦æœ‰æ•ˆ
        if torch.isnan(output).any():
            print("âŒ è­¦å‘Š: è¾“å‡ºåŒ…å«NaN")
        elif torch.isinf(output).any():
            print("âŒ è­¦å‘Š: è¾“å‡ºåŒ…å«Inf")
        else:
            print("âœ… è¾“å‡ºæ•°å€¼æ­£å¸¸")
        
    except Exception as e:
        print(f"âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # æ€§èƒ½æµ‹è¯•
    print("\nâ±ï¸ æ€§èƒ½æµ‹è¯•...")
    import time
    num_runs = 10
    
    times = []
    with torch.no_grad():
        for _ in range(num_runs):
            start = time.time()
            _ = model(x, time, cond_input=cond_input)
            if device.type == 'cuda':
                torch.cuda.synchronize()
            times.append(time.time() - start)
    
    avg_time = sum(times) / len(times)
    print(f"   å¹³å‡æ¨ç†æ—¶é—´: {avg_time*1000:.2f}ms")
    print(f"   ååé‡: {batch_size/avg_time:.1f} samples/s")
    
    # å†…å­˜ä½¿ç”¨
    if device.type == 'cuda':
        memory_mb = torch.cuda.max_memory_allocated() / 1024**2
        print(f"   å³°å€¼GPUå†…å­˜: {memory_mb:.1f}MB")
    
    print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    return True

def compare_models():
    """å¯¹æ¯”åŸå§‹å’Œä¼˜åŒ–æ¨¡å‹"""
    print("\nğŸ”„ å¯¹æ¯”åŸå§‹æ¨¡å‹å’Œä¼˜åŒ–æ¨¡å‹")
    print("=" * 50)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # æµ‹è¯•æ•°æ®
    x = torch.randn(2, 3, 50).to(device)
    time = torch.randint(0, 1000, (2,)).to(device)
    cond_input = torch.randn(2, 10).to(device)
    
    model_config = {
        'dim': 32,
        'init_dim': 32,
        'dim_mults': (1, 2),
        'channels': 3,
        'cond_dim': 10
    }
    
    try:
        # åŸå§‹æ¨¡å‹
        from Unet_with_condition import Unet1D as OriginalUnet
        original_model = OriginalUnet(**model_config).to(device)
        original_model.eval()
        
        # ä¼˜åŒ–æ¨¡å‹
        from Unet_with_condition_optimized import Unet1D as OptimizedUnet
        optimized_model = OptimizedUnet(**model_config).to(device)
        optimized_model.eval()
        
        # æµ‹è¯•è¾“å‡ºä¸€è‡´æ€§
        print("ğŸ” æµ‹è¯•è¾“å‡ºä¸€è‡´æ€§...")
        with torch.no_grad():
            # å¤åˆ¶æƒé‡ç¡®ä¿å…¬å¹³å¯¹æ¯”
            optimized_model.load_state_dict(original_model.state_dict())
            
            out_original = original_model(x, time, cond_input=cond_input)
            out_optimized = optimized_model(x, time, cond_input=cond_input)
        
        diff = torch.abs(out_original - out_optimized).max().item()
        print(f"   æœ€å¤§å·®å¼‚: {diff:.6f}")
        
        if diff < 1e-4:
            print("âœ… è¾“å‡ºå®Œå…¨ä¸€è‡´")
        elif diff < 1e-2:
            print("âœ… è¾“å‡ºåŸºæœ¬ä¸€è‡´")
        else:
            print(f"âš ï¸ è¾“å‡ºå­˜åœ¨å·®å¼‚: {diff}")
        
        # æ€§èƒ½å¯¹æ¯”
        import time
        print("\nâ±ï¸ æ€§èƒ½å¯¹æ¯”...")
        num_runs = 20
        
        # åŸå§‹æ¨¡å‹
        times = []
        with torch.no_grad():
            for _ in range(num_runs):
                start = time.time()
                _ = original_model(x, time, cond_input=cond_input)
                if device.type == 'cuda':
                    torch.cuda.synchronize()
                times.append(time.time() - start)
        original_time = sum(times) / len(times)
        
        # ä¼˜åŒ–æ¨¡å‹
        times = []
        with torch.no_grad():
            for _ in range(num_runs):
                start = time.time()
                _ = optimized_model(x, time, cond_input=cond_input)
                if device.type == 'cuda':
                    torch.cuda.synchronize()
                times.append(time.time() - start)
        optimized_time = sum(times) / len(times)
        
        speedup = original_time / optimized_time
        
        print(f"   åŸå§‹æ¨¡å‹: {original_time*1000:.2f}ms")
        print(f"   ä¼˜åŒ–æ¨¡å‹: {optimized_time*1000:.2f}ms")
        print(f"   åŠ é€Ÿæ¯”: {speedup:.2f}x")
        
        if speedup > 1.5:
            print("ğŸ‰ æ˜¾è‘—åŠ é€Ÿï¼")
        elif speedup > 1.1:
            print("âœ… æ€§èƒ½æå‡")
        elif speedup > 0.9:
            print("âš ï¸ æ€§èƒ½ç›¸å½“")
        else:
            print("âš ï¸ æ€§èƒ½ä¸‹é™ï¼ˆå¯èƒ½æ˜¯å›é€€åˆ°PyTorchå®ç°ï¼‰")
        
    except Exception as e:
        print(f"âŒ å¯¹æ¯”æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def main():
    print("ğŸ¯ UNetä¼˜åŒ–æ¨¡å‹æµ‹è¯•å¥—ä»¶")
    print("=" * 50)
    
    # åŸºæœ¬æµ‹è¯•
    if test_model():
        # å¯¹æ¯”æµ‹è¯•
        try:
            compare_models()
        except Exception as e:
            print(f"âš ï¸ å¯¹æ¯”æµ‹è¯•è·³è¿‡: {e}")
        
        print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
        print("\nğŸ’¡ æ€»ç»“:")
        print("   - ä¼˜åŒ–æ¨¡å‹å·¥ä½œæ­£å¸¸")
        print("   - å¯ä»¥ç›´æ¥æ›¿æ¢åŸå§‹æ¨¡å‹ä½¿ç”¨")
        print("   - å¦‚æœæœ‰C++æ‰©å±•ä¼šè‡ªåŠ¨åŠ é€Ÿ")
        print("   - æ²¡æœ‰C++æ‰©å±•ä¹Ÿèƒ½æ­£å¸¸å·¥ä½œ")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥")

if __name__ == "__main__":
    main()

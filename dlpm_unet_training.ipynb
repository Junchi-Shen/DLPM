{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: einops in /Users/junchishen/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages (0.8.1)\n",
            "Requirement already satisfied: accelerate in /Users/junchishen/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages (1.12.0)\n",
            "Requirement already satisfied: ema-pytorch in /Users/junchishen/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages (0.7.9)\n",
            "Requirement already satisfied: denoising-diffusion-pytorch in /Users/junchishen/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages (2.2.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/junchishen/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages (from accelerate) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/junchishen/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in /Users/junchishen/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages (from accelerate) (7.1.3)\n",
            "Requirement already satisfied: pyyaml in /Users/junchishen/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /Users/junchishen/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages (from accelerate) (2.2.2)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /Users/junchishen/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages (from accelerate) (1.2.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /Users/junchishen/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages (from accelerate) (0.7.0)\n",
            "Requirement already satisfied: pillow in /Users/junchishen/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages (from denoising-diffusion-pytorch) (12.0.0)\n",
            "Requirement already satisfied: pytorch-fid in /Users/junchishen/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages (from denoising-diffusion-pytorch) (0.3.0)\n",
            "Requirement already satisfied: scipy in /Users/junchishen/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages (from denoising-diffusion-pytorch) (1.15.3)\n",
            "Requirement already satisfied: torchvision in /Users/junchishen/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages (from denoising-diffusion-pytorch) (0.17.2)\n",
            "Requirement already satisfied: tqdm in /Users/junchishen/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages (from denoising-diffusion-pytorch) (4.67.1)\n",
            "Requirement already satisfied: filelock in /Users/junchishen/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/junchishen/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.12.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /Users/junchishen/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/junchishen/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /Users/junchishen/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.5.4)\n",
            "Requirement already satisfied: typer-slim in /Users/junchishen/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (0.21.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/junchishen/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
            "Requirement already satisfied: anyio in /Users/junchishen/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (4.12.0)\n",
            "Requirement already satisfied: certifi in /Users/junchishen/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/junchishen/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (1.0.9)\n",
            "Requirement already satisfied: idna in /Users/junchishen/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /Users/junchishen/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (0.16.0)\n",
            "Requirement already satisfied: sympy in /Users/junchishen/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx in /Users/junchishen/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /Users/junchishen/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/junchishen/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/junchishen/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/junchishen/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /Users/junchishen/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages (from typer-slim->huggingface_hub>=0.21.0->accelerate) (8.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install einops accelerate ema-pytorch denoising-diffusion-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 从 DLPM_model 中导入 1D Unet、Dataset1D 和 DLPM 噪声采样函数\n",
        "from DLPM_model import Unet1D, Dataset1D, sample_dlpm_noise_like\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "示例价格序列长度: 23\n"
          ]
        }
      ],
      "source": [
        "# 数据预处理：读取 CSV 并构造 price_series 的对数收益率序列\n",
        "\n",
        "import re\n",
        "\n",
        "# 读取原始数据（路径和 unet_model.ipynb 保持一致）\n",
        "train_data = pd.read_csv('trainning_data_merged.csv')\n",
        "\n",
        "# 去掉重复逻辑的列\n",
        "if 'ticker' in train_data.columns:\n",
        "    train_data.drop(columns=['ticker'], inplace=True)\n",
        "\n",
        "# 归一化日期相关特征\n",
        "if 'contract_calendar_days' in train_data.columns:\n",
        "    train_data['contract_calendar_days'] = train_data['contract_calendar_days'] / 365.0\n",
        "if 'actual_trading_days' in train_data.columns:\n",
        "    train_data['actual_trading_days'] = train_data['actual_trading_days'] / 252.0\n",
        "    train_data['trading_ratio'] = train_data['actual_trading_days'] / train_data['contract_calendar_days']\n",
        "\n",
        "# 解析 price_series 字符串为 tensor\n",
        "\n",
        "def parse_price_series(price_list):\n",
        "    \"\"\"将字符串形式的价格序列解析为 float32 的 torch.tensor\"\"\"\n",
        "    combined = ''.join(str(item) for item in price_list)\n",
        "    numbers = re.findall(r'\\d+\\.\\d+|\\d+', combined)\n",
        "    return torch.tensor([float(n) for n in numbers], dtype=torch.float32)\n",
        "\n",
        "train_data['price_series'] = train_data['price_series'].apply(parse_price_series)\n",
        "\n",
        "print(\"示例价格序列长度:\", len(train_data['price_series'].iloc[0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "targets 形状: torch.Size([65496, 252])\n",
            "masks 形状: torch.Size([65496, 252])\n",
            "seq_tensor 形状: torch.Size([65496, 1, 252])\n"
          ]
        }
      ],
      "source": [
        "# 将多行 price_series 转换为固定长度的对数收益率序列 (targets) 和 mask\n",
        "\n",
        "def process_multiple_rows(price_series_column, max_length=252):\n",
        "    \"\"\"\n",
        "    输入:\n",
        "        price_series_column: pandas Series，每个元素是一个价格序列的 tensor\n",
        "        max_length: 序列总长度（包含起始标记），默认为 252\n",
        "    输出:\n",
        "        targets: (N, max_length) 的 tensor，每行是 [start_flag, log_returns...]\n",
        "        masks:   (N, max_length) 的 tensor，1 表示有效位置\n",
        "    \"\"\"\n",
        "    n_samples = len(price_series_column)\n",
        "    targets = torch.zeros(n_samples, max_length, dtype=torch.float32)\n",
        "    masks = torch.zeros(n_samples, max_length, dtype=torch.float32)\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        price_seq = price_series_column.iloc[i]\n",
        "        log_returns = torch.diff(torch.log(price_seq))  # (T-1,)\n",
        "\n",
        "        # 起始标记\n",
        "        targets[i, 0] = 1.0\n",
        "        masks[i, 0] = 1.0\n",
        "\n",
        "        num_to_fill = min(len(log_returns), max_length - 1)\n",
        "        targets[i, 1:1+num_to_fill] = log_returns[:num_to_fill]\n",
        "        masks[i, 1:1+num_to_fill] = 1.0\n",
        "\n",
        "    return targets, masks\n",
        "\n",
        "max_seq_len = 252\n",
        "\n",
        "targets, masks = process_multiple_rows(train_data['price_series'], max_length=max_seq_len)\n",
        "print(\"targets 形状:\", targets.shape)\n",
        "print(\"masks 形状:\", masks.shape)\n",
        "\n",
        "# 把 targets 扩成 (N, 1, L)，作为 Unet1D 的输入/输出序列\n",
        "seq_tensor = targets.unsqueeze(1).to(device)  # (N, 1, L)\n",
        "print(\"seq_tensor 形状:\", seq_tensor.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gamma_bar 形状: torch.Size([1000])\n",
            "sigma_bar 形状: torch.Size([1000])\n"
          ]
        }
      ],
      "source": [
        "# DLPM 余弦调度：gamma_bar, sigma_bar\n",
        "\n",
        "def get_dlpm_cosine_schedule(T, alpha, s=0.008):\n",
        "    \"\"\"对应 notebook 中的 DLPM 余弦调度，返回 gamma_bar, sigma_bar（长度 T）\"\"\"\n",
        "    steps = np.arange(T + 1)\n",
        "    ft = np.cos(((steps / T) + s) / (1 + s) * (np.pi / 2)) ** 2\n",
        "    alphas_bar = ft / ft[0]\n",
        "\n",
        "    gamma_1_to_t = alphas_bar ** (1.0 / alpha)\n",
        "    sigma_1_to_t = (1.0 - alphas_bar) ** (1.0 / alpha)\n",
        "\n",
        "    return gamma_1_to_t[1:], sigma_1_to_t[1:]\n",
        "\n",
        "# 超参数\n",
        "T = 1000          # 总时间步\n",
        "alpha = 1.0       # DLPM 尾部指数（可以改成 1.5 / 1.7 等）\n",
        "\n",
        "# 预计算调度，并转成 torch.tensor 放在当前设备\n",
        "_gamma_bar_np, _sigma_bar_np = get_dlpm_cosine_schedule(T, alpha)\n",
        "\n",
        "gamma_bar = torch.tensor(_gamma_bar_np, dtype=torch.float32, device=device)  # (T,)\n",
        "sigma_bar = torch.tensor(_sigma_bar_np, dtype=torch.float32, device=device)  # (T,)\n",
        "\n",
        "print(\"gamma_bar 形状:\", gamma_bar.shape)\n",
        "print(\"sigma_bar 形状:\", sigma_bar.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "数据集大小: 65496\n"
          ]
        }
      ],
      "source": [
        "# 构造 Dataset 和 DataLoader\n",
        "\n",
        "full_dataset = Dataset1D(seq_tensor)  # 每个样本形状 (1, L)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "print(\"数据集大小:\", len(full_dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "模型参数量: 4.580225 M\n"
          ]
        }
      ],
      "source": [
        "# 定义最基础的 1D U-Net 模型（使用 DLPM_model 中的 Unet1D）\n",
        "\n",
        "model = Unet1D(\n",
        "    dim=64,               # 基础通道数，可以根据显存调整\n",
        "    dim_mults=(1, 2, 4),  # 三层下采样/上采样\n",
        "    channels=1,           # 序列是 1 通道\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "print(\"模型参数量:\", sum(p.numel() for p in model.parameters()) / 1e6, \"M\")\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 使用 testing_data_merged.csv 做评估：先按同样逻辑预处理，得到测试集序列\n",
        "\n",
        "# 1. 读取测试数据\n",
        "test_data = pd.read_csv('testing_data_merged.csv')\n",
        "\n",
        "# 2. 与训练集保持一致的预处理\n",
        "if 'ticker' in test_data.columns:\n",
        "    test_data.drop(columns=['ticker'], inplace=True)\n",
        "\n",
        "if 'contract_calendar_days' in test_data.columns:\n",
        "    test_data['contract_calendar_days'] = test_data['contract_calendar_days'] / 365.0\n",
        "if 'actual_trading_days' in test_data.columns:\n",
        "    test_data['actual_trading_days'] = test_data['actual_trading_days'] / 252.0\n",
        "    test_data['trading_ratio'] = test_data['actual_trading_days'] / test_data['contract_calendar_days']\n",
        "\n",
        "# 3. 解析 price_series\n",
        "test_data['price_series'] = test_data['price_series'].apply(parse_price_series)\n",
        "\n",
        "# 4. 构造对数收益率序列\n",
        "test_targets, test_masks = process_multiple_rows(test_data['price_series'], max_length=max_seq_len)\n",
        "print('test_targets 形状:', test_targets.shape)\n",
        "print('test_masks  形状:', test_masks.shape)\n",
        "\n",
        "# 5. 扩成 (N_test, 1, L)\n",
        "test_seq_tensor = test_targets.unsqueeze(1).to(device)\n",
        "print('test_seq_tensor 形状:', test_seq_tensor.shape)\n",
        "\n",
        "test_dataset = Dataset1D(test_seq_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "print('测试集大小:', len(test_dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5] Step [100] Loss: 30.726323\n",
            "Epoch [1/5] Step [200] Loss: 18.552368\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# t 需要是 (B,) 的 long 类型\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m pred_epsilon \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# 6) loss：和你 notebook 保持一致，预测的是 A_half * G_t\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#    error = model(Y_t, t) - g_noise * A_half\u001b[39;00m\n\u001b[1;32m     40\u001b[0m target \u001b[38;5;241m=\u001b[39m epsilon_t  \u001b[38;5;66;03m# 等价于 g_noise * A_half\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/DLPM/DLPM_model.py:456\u001b[0m, in \u001b[0;36mUnet1D.forward\u001b[0;34m(self, x, time, x_self_cond)\u001b[0m\n\u001b[1;32m    452\u001b[0m     x \u001b[38;5;241m=\u001b[39m upsample(x)\n\u001b[1;32m    454\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x, r), dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 456\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinal_res_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_conv(x)\n",
            "File \u001b[0;32m~/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/DLPM/DLPM_model.py:266\u001b[0m, in \u001b[0;36mResnetBlock.forward\u001b[0;34m(self, x, time_emb)\u001b[0m\n\u001b[1;32m    263\u001b[0m     time_emb \u001b[38;5;241m=\u001b[39m rearrange(time_emb, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb c -> b c 1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    264\u001b[0m     scale_shift \u001b[38;5;241m=\u001b[39m time_emb\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 266\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_shift\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscale_shift\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock2(h)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m h \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres_conv(x)\n",
            "File \u001b[0;32m~/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/DLPM/DLPM_model.py:236\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, scale_shift)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, scale_shift \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 236\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exists(scale_shift):\n",
            "File \u001b[0;32m~/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages/torch/nn/modules/conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/dlpm_clean/lib/python3.10/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    304\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    305\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 训练循环：严格对应你 notebook 中的\n",
        "# error = model(Y_t, t) - g_noise * A_half\n",
        "# 的 DLPM 噪声逻辑\n",
        "\n",
        "num_epochs = 5          # 可以先设小一点测试\n",
        "print_every = 100\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for step, x0 in enumerate(train_loader):\n",
        "        # x0: (B, 1, L)\n",
        "        x0 = x0.to(device)\n",
        "        B = x0.size(0)\n",
        "\n",
        "        # 1) 随机采样时间步 t ~ Uniform({0, ..., T-1})\n",
        "        t = torch.randint(0, T, (B,), device=device, dtype=torch.long)  # (B,)\n",
        "\n",
        "        # 2) 根据 t 取出对应的 gamma_t, sigma_t，并调整维度用于广播\n",
        "        gamma_t = gamma_bar[t].view(B, 1, 1)  # (B,1,1)\n",
        "        sigma_t = sigma_bar[t].view(B, 1, 1)  # (B,1,1)\n",
        "\n",
        "        # 3) 生成 DLPM 重尾噪声 epsilon_t = A_half * G_t\n",
        "        #    sample_dlpm_noise_like 返回 (epsilon, G_t, A_half)\n",
        "        epsilon_t, g_noise, A_half = sample_dlpm_noise_like(x0, alpha=alpha)\n",
        "\n",
        "        # 4) 按 DLPM 公式生成 Y_t\n",
        "        #    Y_t = gamma_t * Y_0 + sigma_t * epsilon_t\n",
        "        Y_t = gamma_t * x0 + sigma_t * epsilon_t\n",
        "\n",
        "        # 5) 前向：模型输入 (Y_t, t)，预测 epsilon_t\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # t 需要是 (B,) 的 long 类型\n",
        "        pred_epsilon = model(Y_t, t)\n",
        "\n",
        "        # 6) loss：和你 notebook 保持一致，预测的是 A_half * G_t\n",
        "        #    error = model(Y_t, t) - g_noise * A_half\n",
        "        target = epsilon_t  # 等价于 g_noise * A_half\n",
        "        error = pred_epsilon - target\n",
        "        mse = torch.mean(error ** 2)\n",
        "        loss = torch.sqrt(mse)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if (step + 1) % print_every == 0:\n",
        "            avg_loss = running_loss / print_every\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}] Step [{step+1}] Loss: {avg_loss:.6f}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "print(\"训练结束！\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 在测试集上做“反推”评估：给定 Y_0，按 DLPM 正向加噪得到 Y_t，\n",
        "# 再用模型预测 epsilon_t，并反推 Y_0_hat，与真实 Y_0 比较\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model.eval()\n",
        "\n",
        "all_mse = []\n",
        "recon_examples = []  # 保存少量样本做可视化\n",
        "\n",
        "# 选一个评估用的固定时间步，比如中间的 t_eval\n",
        "t_eval = T // 2\n",
        "print(\"使用评估时间步 t_eval =\", t_eval)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, x0 in enumerate(test_loader):\n",
        "        # x0: (B, 1, L)\n",
        "        x0 = x0.to(device)\n",
        "        B = x0.size(0)\n",
        "\n",
        "        # 1) 构造 batch 形式的时间步索引\n",
        "        t_batch = torch.full((B,), t_eval, device=device, dtype=torch.long)  # (B,)\n",
        "\n",
        "        # 2) 取出对应的 gamma_t, sigma_t\n",
        "        gamma_t = gamma_bar[t_batch].view(B, 1, 1)\n",
        "        sigma_t = sigma_bar[t_batch].view(B, 1, 1)\n",
        "\n",
        "        # 3) 生成 DLPM 重尾噪声 epsilon_t = A_half * G_t\n",
        "        epsilon_t, g_noise, A_half = sample_dlpm_noise_like(x0, alpha=alpha)\n",
        "\n",
        "        # 4) 正向加噪，得到 Y_t\n",
        "        Y_t = gamma_t * x0 + sigma_t * epsilon_t\n",
        "\n",
        "        # 5) 模型预测 epsilon_t\n",
        "        pred_epsilon = model(Y_t, t_batch)\n",
        "\n",
        "        # 6) 根据公式反推 Y_0_hat:\n",
        "        #    Y_t = gamma_t * Y_0 + sigma_t * epsilon_t\n",
        "        # => Y_0_hat = (Y_t - sigma_t * pred_epsilon) / gamma_t\n",
        "        Y0_hat = (Y_t - sigma_t * pred_epsilon) / gamma_t\n",
        "\n",
        "        # 7) 计算该 batch 的 MSE\n",
        "        batch_mse = torch.mean((Y0_hat - x0) ** 2, dim=[1, 2])  # (B,)\n",
        "        all_mse.append(batch_mse.cpu())\n",
        "\n",
        "        # 只保存前几个样本做可视化\n",
        "        if batch_idx < 3:  # 取前 3 个 batch 各 1 条\n",
        "            recon_examples.append((x0[0].detach().cpu(), Y0_hat[0].detach().cpu()))\n",
        "\n",
        "# 汇总 MSE\n",
        "all_mse = torch.cat(all_mse, dim=0)\n",
        "print(\"测试集重建 MSE 均值:\", all_mse.mean().item())\n",
        "print(\"测试集重建 MSE 中位数:\", all_mse.median().item())\n",
        "\n",
        "# 可视化部分样本的原始 vs 重建对数收益率序列\n",
        "num_plots = len(recon_examples)\n",
        "plt.figure(figsize=(12, 4 * num_plots))\n",
        "\n",
        "for i, (x_true, x_hat) in enumerate(recon_examples):\n",
        "    plt.subplot(num_plots, 1, i + 1)\n",
        "    plt.plot(x_true.squeeze().numpy(), label='True log-returns', alpha=0.8)\n",
        "    plt.plot(x_hat.squeeze().numpy(), label='Reconstructed log-returns', alpha=0.8)\n",
        "    plt.title(f'Sample {i+1}: True vs Reconstructed (t={t_eval})')\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dlpm_clean",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

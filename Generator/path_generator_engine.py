# path_generator_engine.py
# 
# è¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„è·¯å¾„ç”Ÿæˆå¼•æ“ã€‚
# å®ƒè¢«è®¾è®¡ä¸º "è¢«å¯¼å…¥"ï¼Œè€Œä¸æ˜¯ "è¢«æ‰§è¡Œ"ã€‚

import sys
from pathlib import Path
import os
import numpy as np
import pandas as pd
from tqdm import tqdm
import torch
import gc
import time
import traceback
import logging
import json


# --- å¯¼å…¥è‡ªå®šä¹‰æ¨¡å— ---
import Generator.path_simulators as ps

# --- è·¯å¾„å¯¼å…¥ ---
try:
    current_file_dir = Path(__file__).parent.resolve()
    project_root = current_file_dir.parent
    sys.path.append(str(project_root))
    # åŠ¨æ€å¯¼å…¥ Project_Path ä¸­çš„æ‰€æœ‰è·¯å¾„
    import Project_Path as pp
    from .path_simulators import load_diffusion_artifacts, simulate_gbm, simulate_garch,run_diffusion_mega_batch
    # éœ€è¦ DataProcessor ç±»å®šä¹‰æ¥åŠ è½½ .pkl æ–‡ä»¶
    from Data.Input_preparation import DataProcessor
except (ImportError, NameError):
    print("âŒ ä¸¥é‡é”™è¯¯: æœªæ‰¾åˆ° Project_Path.pyã€‚å¼•æ“æ— æ³•å·¥ä½œã€‚")
    sys.exit(1)


class PathGeneratorEngine: # é‡å‘½åç±»ä»¥ç¤ºæ¸…æ™°
    """
    é€šç”¨ã€é…ç½®é©±åŠ¨çš„è·¯å¾„ç”Ÿæˆå¼•æ“ (å·²æ›´æ–°ä»¥é›†æˆæ¡ä»¶ç½‘ç»œå’ŒåŠ è½½çš„å¤„ç†å™¨)ã€‚
    """
    def __init__(self, asset_name: str, job_spec: dict, device: str,num_paths_override: int | None = None):
        self.asset_name = asset_name
        self.spec = job_spec
        self.device = device
        self.num_paths_override = num_paths_override
        self.job_type = self.spec['type']
        # ä» spec è·å–ä½œä¸šåï¼Œå¦‚æœæ²¡æœ‰åˆ™æä¾›é»˜è®¤å€¼
        self.job_name = self.spec.get('job_name', f"Unnamed_{self.job_type}_Job")

        print(f"ğŸš€ åˆå§‹åŒ–è·¯å¾„ç”Ÿæˆå¼•æ“ (ä½œä¸š: '{self.job_name}', ç±»å‹: {self.job_type})...")
        self.actual_num_paths = self._determine_num_paths()
        print(f"   å°†ä¸ºæ¯ä¸ªæ¡ä»¶ç”Ÿæˆ {self.actual_num_paths:,} æ¡è·¯å¾„ã€‚")
        
        # å ä½ç¬¦
        self.val_df = None
        self.conditions_df = None # å°†ä» val_df ä¸­é€‰å–
        self.garch_params = None
        self.diffusion_model = None # å°†æŒæœ‰ GaussianDiffusion1D å®ä¾‹
        self.data_processor = None # å°†æŒæœ‰åŠ è½½çš„ DataProcessor å®ä¾‹

        self._load_validation_data() # åŠ è½½æ‰€æœ‰ç±»å‹éƒ½éœ€è¦çš„ val_df

        # --- !! å·²ä¿®æ”¹ï¼šå¦‚æœéœ€è¦ï¼ŒåŠ è½½ GARCH å‚æ•° !! ---
        if self.job_type == 'mc' and 'garch_params_filename' in self.spec:
            self._load_garch_params()
        # ---

        # --- !! å·²ä¿®æ”¹ï¼šå¦‚æœéœ€è¦ï¼ŒåŠ è½½ Diffusion äº§å‡ºç‰© !! ---
        if self.job_type == 'diffusion':
            self._load_diffusion_artifacts()
        # ---

        print("âœ… å¼•æ“åˆå§‹åŒ–å®Œæˆã€‚")
    def _determine_num_paths(self) -> int:
        """æ ¹æ®è¦†ç›–å€¼æˆ–é…ç½®ç¡®å®šæœ€ç»ˆè¦ç”Ÿæˆçš„è·¯å¾„æ•°"""
        if self.num_paths_override is not None and self.num_paths_override > 0:
            print(f"   æ”¶åˆ°å¯åŠ¨å™¨è¦†ç›–å‚æ•°ï¼šnum_paths = {self.num_paths_override}")
            # æ›´æ–° spec å†…éƒ¨çš„å€¼ï¼Œä»¥ä¾¿åç»­å‡½æ•°ä½¿ç”¨æ­£ç¡®çš„å€¼
            if self.job_type == 'mc':
                 self.spec['params']['n_simulations'] = self.num_paths_override
            elif self.job_type == 'diffusion':
                 self.spec['generation_params']['num_paths_to_generate'] = self.num_paths_override
            return self.num_paths_override
        else:
            # ä»é…ç½®ä¸­è·å–é»˜è®¤å€¼
            default_paths = 0
            if self.job_type == 'mc':
                default_paths = self.spec.get('params', {}).get('n_simulations', 1024) # é»˜è®¤ 1024
            elif self.job_type == 'diffusion':
                default_paths = self.spec.get('generation_params', {}).get('num_paths_to_generate', 1024) # é»˜è®¤ 1024
            print(f"   ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤è·¯å¾„æ•°ï¼šnum_paths = {default_paths}")
            return default_paths
        
    def _load_validation_data(self):
        """
        åŠ è½½éªŒè¯æ•°æ® (val_df) ä½œä¸ºç”Ÿæˆæ¡ä»¶ã€‚
        (å·²ä¿®æ”¹ï¼šä»ä¸­å¤® merge æ–‡ä»¶åŠ è½½å¹¶æŒ‰ asset_name è¿‡æ»¤)
        """
        print("ğŸ”„ æ­£åœ¨åŠ è½½ *ä¸­å¤®* éªŒè¯æ•°æ® (testing_data_merged.csv)...")
        val_dir_key = "Testing_DATA_DIR"
        val_base_dir = getattr(pp, val_dir_key, None)
        if val_base_dir is None: raise AttributeError(f"Project_Path.py ç¼ºå°‘ '{val_dir_key}'")

        # --- 1. åŠ è½½ä¸­å¤® merge æ–‡ä»¶ ---
        # å‡è®¾ä¸­å¤®æ–‡ä»¶å (å¦‚æœä½ çš„æ–‡ä»¶åä¸åŒï¼Œè¯·åœ¨æ­¤å¤„ä¿®æ”¹)
        central_file_name = 'testing_data_merged.csv'
        val_file_path = val_base_dir / central_file_name
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not val_file_path.exists():
            # å¤‡ç”¨ï¼šä¸‡ä¸€æ–‡ä»¶åæ˜¯ val_df.csv
            val_file_path_fallback = val_base_dir / 'val_df.csv'
            if val_file_path_fallback.exists():
                val_file_path = val_file_path_fallback
            else:
                 raise FileNotFoundError(f"ä¸­å¤® merge æ–‡ä»¶æœªæ‰¾åˆ°: {val_file_path} æˆ– {val_file_path_fallback}")

        try:
            full_val_df = pd.read_csv(val_file_path)
            print(f"âœ… ä¸­å¤®éªŒè¯æ–‡ä»¶åŠ è½½æˆåŠŸ: {val_file_path}")

            # --- 2. å…³é”®ï¼šè¿‡æ»¤å­é›† ---
            
            # ç¡®å®šè¦è¿‡æ»¤å“ªä¸ªèµ„äº§
            asset_to_filter = self.asset_name
            if self.asset_name.lower() == 'all':
                # 'all' æ¨¡å‹ä¹Ÿéœ€è¦ä¸€ä¸ª *å…·ä½“* èµ„äº§çš„æ¡ä»¶æ¥ç”Ÿæˆ
                asset_to_filter = self.spec.get('representative_asset_for_val', 'CSI1000') 
                print(f"   âš ï¸ 'all' æ¨¡å‹ä½œä¸šå°†ä½¿ç”¨ä»£è¡¨æ€§èµ„äº§ '{asset_to_filter}' çš„æ¡ä»¶ã€‚")

            # !! å…³é”®å‡è®¾ !! 
            # å‡è®¾ç”¨äºè¿‡æ»¤çš„åˆ—åæ˜¯ 'asset_name'
            # å¦‚æœä½ çš„åˆ—åä¸åŒ (ä¾‹å¦‚ 'index')ï¼Œè¯·åœ¨æ­¤å¤„ä¿®æ”¹
            filter_column_name = 'asset_underlying' 
            
            print(f"   ğŸ”„ æ­£åœ¨ä»ä¸­å¤®æ–‡ä»¶ä¸­è¿‡æ»¤ '{asset_to_filter}' çš„å­é›† (åŸºäºåˆ— '{filter_column_name}')...")

            # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
            if filter_column_name not in full_val_df.columns:
                 raise KeyError(f"ä¸­å¤® merge æ–‡ä»¶ '{val_file_path}' ä¸­ç¼ºå°‘ç”¨äºè¿‡æ»¤çš„åˆ—: '{filter_column_name}'")
                 
            self.conditions_df = full_val_df[full_val_df[filter_column_name] == asset_to_filter].copy()
            
            if self.conditions_df.empty:
                raise ValueError(f"åœ¨ '{val_file_path}' ä¸­æ‰¾ä¸åˆ°èµ„äº§ '{asset_to_filter}' çš„ä»»ä½•æ•°æ®ã€‚")

            # åœ¨æ­¤å¼•æ“ä¸­ï¼Œval_df å’Œ conditions_df ç›¸åŒ
            self.val_df = self.conditions_df 
            print(f"âœ… éªŒè¯å­é›†åŠ è½½æˆåŠŸã€‚å°†ä½¿ç”¨ {len(self.conditions_df)} ä¸ª '{asset_to_filter}' å¸‚åœºæ¡ä»¶ã€‚")
        
        except Exception as e:
            print(f"âŒ åŠ è½½æˆ–å¤„ç†ä¸­å¤® merge æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            raise
    # --- !! æ–°æ–¹æ³•ï¼šåŠ è½½ GARCH å‚æ•° !! ---
    def _load_garch_params(self):
        """ä» JSON åŠ è½½é¢„å…ˆæ‹Ÿåˆçš„ GARCH å‚æ•°ã€‚"""
        print("ğŸ”„ æ­£åœ¨åŠ è½½é¢„æ‹Ÿåˆçš„ GARCH å‚æ•°...")
        filename = self.spec['garch_params_filename']
        # å‚æ•°é€šå¸¸ä¿å­˜åœ¨ Model/Garch_Model/<asset>/filename
        params_root_key = self.spec.get('garch_params_dir_key', 'Model_Results_DIR') # é»˜è®¤ Model_Results_DIR
        params_subfolder = self.spec.get('garch_params_subfolder', 'Garch_Fit_Results') # é»˜è®¤ Garch_Fit_Results
        params_root_dir = getattr(pp, params_root_key, None)
        if params_root_dir is None:
            raise AttributeError(f"Project_Path.py ç¼ºå°‘ '{params_root_key}'")
        params_base_dir = params_root_dir / params_subfolder

        params_path = None
        # GARCH å‚æ•°é€šå¸¸æ˜¯ç‰¹å®šäºèµ„äº§çš„ï¼Œå³ä½¿åœ¨ 'all' è®­ç»ƒä¸Šä¸‹æ–‡ä¸­ä¹Ÿå¯èƒ½å¦‚æ­¤
        # å‡è®¾å‚æ•°æ–‡ä»¶æ€»æ˜¯æŒ‰èµ„äº§å­˜å‚¨
        asset_for_garch_params = self.asset_name
        if self.asset_name.lower() == 'all':
             # å¦‚æœæ˜¯ 'all' ä½œä¸šï¼Œéœ€è¦å†³å®šåŠ è½½å“ªä¸ªèµ„äº§çš„ GARCH å‚æ•°
             # å¯èƒ½éœ€è¦ä»é…ç½®æŒ‡å®šï¼Œæˆ–ä½¿ç”¨é»˜è®¤å€¼
             asset_for_garch_params = self.spec.get('representative_asset_for_garch', 'CSI1000') # ç¤ºä¾‹
             print(f"   âš ï¸ ä½¿ç”¨ä»£è¡¨æ€§èµ„äº§ '{asset_for_garch_params}' çš„ GARCH å‚æ•°ä¸º 'all' ä½œä¸šã€‚")

        params_path = params_base_dir / asset_for_garch_params / filename

        if not params_path.exists():
            raise FileNotFoundError(
                f"GARCH å‚æ•°æ–‡ä»¶æœªæ‰¾åˆ°: {params_path}ã€‚"
                f"è¯·å…ˆè¿è¡Œ GARCH æ‹Ÿåˆè„šæœ¬ã€‚"
            )
        try:
            with open(params_path, 'r') as f:
                # åŠ è½½æ—¶å°† null è½¬æ¢å› np.nan
                self.garch_params = json.load(f, object_hook=lambda d: {k: (np.nan if v is None else v) for k, v in d.items()})
            print(f"âœ… ä»ä»¥ä¸‹ä½ç½®åŠ è½½ GARCH å‚æ•°æˆåŠŸ: {params_path}")
            if not all(k in self.garch_params for k in ['omega', 'alpha', 'beta']):
                raise ValueError("åŠ è½½çš„ GARCH å‚æ•°ç¼ºå°‘å¿…éœ€é”®ã€‚")
        except Exception as e:
            print(f"âŒ åŠ è½½æˆ–è§£æ GARCH å‚æ•°æ—¶å‡ºé”™: {e}")
            raise RuntimeError("åŠ è½½ GARCH å‚æ•°å¤±è´¥ã€‚") from e
    # ---

    # --- !! å·²ä¿®æ”¹ï¼šåŠ è½½ Diffusion äº§å‡ºç‰© !! ---
    def _load_diffusion_artifacts(self):
        """åŠ è½½ DataProcessor, CondNet, U-Net, å’Œ Diffusion åŒ…è£…å™¨ã€‚"""
        print("ğŸ”„ æ­£åœ¨åŠ è½½æ‰©æ•£æ¨¡å‹äº§å‡ºç‰©...")
        loader_params = self.spec.get('model_loader_params')
        if not loader_params: raise ValueError("é…ç½®ä¸­ç¼ºå°‘ 'model_loader_params'ã€‚")

        # ç¡®å®šæ¨¡å‹äº§å‡ºç‰©ç›®å½• (é€šå¸¸åœ¨ Results/Model_Results)
        model_dir_root_key = loader_params.get('model_dir_root_key', 'Model_Results_DIR') # é»˜è®¤é”®å
        model_base_dir = getattr(pp, model_dir_root_key, None)
        if model_base_dir is None: raise AttributeError(f"Project_Path.py ç¼ºå°‘ '{model_dir_root_key}'")

        # å¤„ç† 'all' vs ç‰¹å®šèµ„äº§è·¯å¾„
        model_folder_name = self.spec.get('model_source_folder', self.asset_name)
        model_dir = model_base_dir / model_folder_name

        if not model_dir.exists():
            raise FileNotFoundError(f"æ¨¡å‹äº§å‡ºç‰©ç›®å½•æœªæ‰¾åˆ°: {model_dir}ã€‚è¯·ç¡®ä¿å·²ä¸º '{self.asset_name}' è¿è¡Œè®­ç»ƒã€‚")

        try:
            # ä½¿ç”¨ path_simulators ä¸­çš„æ›´æ–°åçš„åŠ è½½å‡½æ•°
            self.diffusion_model, self.data_processor = load_diffusion_artifacts(
                model_dir=model_dir,
                processor_filename=loader_params['processor_filename'],
                model_filename=loader_params['model_filename'],
                # ä½¿ç”¨ .get ä»¥å…è®¸æ¡ä»¶ç½‘ç»œæ˜¯å¯é€‰çš„
                condition_network_filename=loader_params.get('condition_network_filename'),
                unet_config=self.spec['unet_config'],       # ä¼ é€’ unet é…ç½®
                diffusion_config=self.spec['diffusion_config'],  # ä¼ é€’ diffusion é…ç½®
                cond_net_config=self.spec.get('cond_net_config'), # ä¼ é€’æ¡ä»¶ç½‘ç»œé…ç½® (å¯èƒ½æ˜¯ None)
                device=self.device
            )
            print("âœ… æ‰©æ•£æ¨¡å‹äº§å‡ºç‰©åŠ è½½æˆåŠŸã€‚")
            if not isinstance(self.data_processor, DataProcessor):
                 print("   âš ï¸ è­¦å‘Š: åŠ è½½çš„ data_processor ç±»å‹ä¸æ˜¯é¢„æœŸçš„ DataProcessorã€‚")

        except Exception as e:
             print(f"âŒ ä» {model_dir} åŠ è½½æ‰©æ•£äº§å‡ºç‰©æ—¶å‡ºé”™: {e}")
             traceback.print_exc()
             raise RuntimeError("åŠ è½½æ‰©æ•£äº§å‡ºç‰©å¤±è´¥ã€‚") from e
    # ---

    def run(self):
        """æ‰§è¡Œä¸»ç”Ÿæˆé€»è¾‘ã€‚"""
        print(f"\nğŸ å¼€å§‹ç”Ÿæˆä½œä¸š: {self.job_name} ...")
        start_time = time.time()
        
        try:
            if self.job_type == 'mc':
                self._run_mc_generation()
            elif self.job_type == 'diffusion':
                self._run_diffusion_generation()
            else:
                raise ValueError(f"æœªçŸ¥çš„ä½œä¸šç±»å‹: {self.job_type}")
        except Exception as e:
             print(f"âŒ ä½œä¸š '{self.job_name}' æ‰§è¡Œè¿‡ç¨‹ä¸­å¤±è´¥: {e}")
             traceback.print_exc()
             # å³ä½¿å¤±è´¥ä¹Ÿæ‰“å°ç»“æŸä¿¡æ¯
        
        end_time = time.time(); duration = end_time - start_time
        duration_str = time.strftime("%H:%M:%S", time.gmtime(duration))
        print(f"ğŸ ä½œä¸š {self.job_name} ç»“æŸã€‚æ€»ç”¨æ—¶: {duration_str}ã€‚")

    # --- !! å·²ä¿®æ”¹ï¼šMC ç”Ÿæˆé€»è¾‘ !! ---
    def _run_mc_generation(self):
        """ä½¿ç”¨åŠ è½½çš„å‚æ•°æ‰§è¡Œè¿­ä»£å¼ MC (GBM, GARCH) ç”Ÿæˆã€‚"""
        simulator_func = self.spec['simulator_function']
        sim_params = self.spec.get('params', {})
        n_steps_total = sim_params.get('n_steps_total', 252)

        all_paths_list = []
        all_sigmas_list = []
        all_masks_list = []

        is_garch_job = 'garch_params_filename' in self.spec
        if is_garch_job and self.garch_params is None:
             raise RuntimeError("GARCH ä½œä¸šå·²æŒ‡å®šï¼Œä½†å‚æ•°æœªåŠ è½½ã€‚")

        print(f"   æ¨¡æ‹Ÿ {len(self.conditions_df)} ä¸ªæ¡ä»¶...")
        # ç¡®ä¿ T_days æ˜¯æ•´æ•°
        self.conditions_df['actual_trading_days'] = pd.to_numeric(self.conditions_df['actual_trading_days'], errors='coerce').fillna(0).astype(int)

        for _, row in tqdm(self.conditions_df.iterrows(), total=len(self.conditions_df), desc="æ¨¡æ‹Ÿ MC è·¯å¾„"):
            # æ£€æŸ¥æ˜¯å¦æœ‰ NaN æ¡ä»¶
            if row.isnull().any():
                 logging.warning(f"è·³è¿‡åŒ…å« NaN æ¡ä»¶çš„è¡Œ (ç´¢å¼• {_})ã€‚")
                 continue

            params_for_sim = {
                "S0": row['start_price'], "r": row['risk_free_rate'],
                "initial_vol_ann": row['volatility'], "sigma": row['volatility'],
                "T_days": row['actual_trading_days'], # ç¡®ä¿æ˜¯æ•´æ•°
                **sim_params
            }
            if is_garch_job:
                params_for_sim['garch_params'] = self.garch_params
            # --- !! åœ¨è¿™é‡Œæ·»åŠ æ¸…ç†é€»è¾‘ !! ---
            if simulator_func.__name__ == 'simulate_gbm':
                params_for_sim.pop('initial_vol_ann', None) # ç§»é™¤ GARCH ä¸“ç”¨å‚æ•°
            elif simulator_func.__name__ == 'simulate_garch':
                params_for_sim.pop('sigma', None) # ç§»é™¤ GBM ä¸“ç”¨å‚æ•°
        # --- !! æ·»åŠ ç»“æŸ !! ---
            try:
                result = simulator_func(**params_for_sim)
                if isinstance(result, tuple): # GARCH
                    paths_out, sigma2_out = result
                    all_paths_list.append(paths_out)
                    if self.spec.get('save_extra_outputs', False):
                        n_sim, _, current_len = sigma2_out.shape
                        padded_arr = np.full((n_sim, 1, n_steps_total), np.nan, dtype=np.float64)
                        L = min(current_len, n_steps_total); padded_arr[:, :, :L] = sigma2_out[:, :, :L]
                        all_sigmas_list.append(padded_arr)
                        all_masks_list.append(~np.isnan(paths_out))
                else: # GBM
                    paths_out = result
                    all_paths_list.append(paths_out)
            except Exception as sim_e:
                 logging.error(f"æ¨¡æ‹Ÿæ¡ä»¶ {_} æ—¶å¤±è´¥: {sim_e}")
                 # å¯ä»¥é€‰æ‹©å¡«å…… NaN æˆ–è·³è¿‡
                 n_sim = sim_params.get("n_simulations", 1) # è·å– n_sim
                 all_paths_list.append(np.full((n_sim, 1, n_steps_total), np.nan)) # å¡«å…… NaN è·¯å¾„
                 if is_garch_job and self.spec.get('save_extra_outputs', False):
                      all_sigmas_list.append(np.full((n_sim, 1, n_steps_total), np.nan))
                      all_masks_list.append(np.full((n_sim, 1, n_steps_total), False))


        # æ•´åˆç»“æœ - æ²¿æ¡ä»¶ç»´åº¦å †å  (axis=0)
        if not all_paths_list: print("âš ï¸ æ²¡æœ‰æˆåŠŸæ¨¡æ‹Ÿä»»ä½•è·¯å¾„ã€‚"); return
        final_paths_array = np.concatenate(all_paths_list, axis=0)
        extra_arrays_to_save = {}
        if self.spec.get('save_extra_outputs', False):
            if all_sigmas_list: extra_arrays_to_save['sigma2'] = np.concatenate(all_sigmas_list, axis=0)
            if all_masks_list: extra_arrays_to_save['mask'] = np.concatenate(all_masks_list, axis=0)

        self._save_results(final_paths_array, **extra_arrays_to_save)
    # ---

    # --- !! å·²ä¿®æ”¹ï¼šDiffusion ç”Ÿæˆé€»è¾‘ !! ---
    def _run_diffusion_generation(self):
        """æ‰§è¡ŒçŸ¢é‡åŒ– Diffusion (UNet) ç”Ÿæˆã€‚"""
        if self.diffusion_model is None or self.data_processor is None:
            raise RuntimeError("æ‰©æ•£æ¨¡å‹äº§å‡ºç‰©æœªåŠ è½½ã€‚")

        # --- 1. ä½¿ç”¨åŠ è½½çš„ DataProcessor å‡†å¤‡æ¡ä»¶ ---
        print("ğŸ”„ æ­£åœ¨ä½¿ç”¨åŠ è½½çš„ DataProcessor å¤„ç†éªŒè¯é›†æ¡ä»¶...")
        X_test = None
        try:
            # ** å…³é”®ï¼šè°ƒç”¨ create_condition_tensors (fit_scaler=False) **
            # éœ€è¦å…ˆè¿è¡Œ process_price_data æ¥å‡†å¤‡å¿…è¦çš„åˆ— (å¦‚ S_0)
            df_processed_val = self.data_processor.process_price_data(self.conditions_df)
            df_processed_val = df_processed_val.dropna(subset=['S_0', 'price_series']) # ç§»é™¤æ— æ•ˆè¡Œ
            if df_processed_val.empty: raise ValueError("éªŒè¯é›†ä¸­æ²¡æœ‰æœ‰æ•ˆçš„æ¡ä»¶ã€‚")

            # ä½¿ç”¨å·²æ‹Ÿåˆçš„ price_scaler (æ¥è‡ªè®­ç»ƒ) è¿›è¡Œè½¬æ¢
            condition_dict = self.data_processor.create_condition_tensors(df_processed_val, fit_scaler=False)
            X_test_np = condition_dict['conditions'] # 7D numpy array
            X_test = torch.FloatTensor(X_test_np).to(self.device) # è½¬ä¸º Tensor å¹¶ç§»åˆ°è®¾å¤‡
            print(f"âœ… æ¡ä»¶å‡†å¤‡å®Œæ¯•ï¼Œç”¨äºç”Ÿæˆã€‚å½¢çŠ¶: {X_test.shape}")

        except AttributeError as ae:
             # æ•è· DataProcessor æ²¡æœ‰ scaler çš„é”™è¯¯ (å¦‚æœåŠ è½½å¤±è´¥æˆ–æœªè®­ç»ƒ)
             if 'price_scaler' in str(ae) and 'n_features' in str(ae):
                  print("âŒ é”™è¯¯: åŠ è½½çš„ DataProcessor ä¸­çš„ price_scaler ä¼¼ä¹æœªæ‹Ÿåˆã€‚")
                  print("   è¯·ç¡®ä¿ç”¨äºè®­ç»ƒçš„ data_processor.pkl æ–‡ä»¶å·²æ­£ç¡®ä¿å­˜å¹¶åŒ…å«æ‹Ÿåˆçš„æ ‡å‡†åŒ–å™¨ã€‚")
                  raise RuntimeError("DataProcessor scaler æœªæ‹Ÿåˆã€‚") from ae
             else: raise # é‡æ–°æŠ›å‡ºå…¶ä»– AttributeError
        except Exception as e:
             print(f"âŒ ä½¿ç”¨åŠ è½½çš„ DataProcessor å‡†å¤‡æ¡ä»¶æ—¶å‡ºé”™: {e}")
             traceback.print_exc()
             raise RuntimeError("ä¸ºæ‰©æ•£æ¨¡å‹ç”Ÿæˆå‡†å¤‡æ¡ä»¶å¤±è´¥ã€‚") from e

        # --- 2. è·å–è¿è¡Œå™¨å‡½æ•° ---
        runner_func_spec = self.spec['generation_params'].get('runner_function')
        if runner_func_spec is None: raise ValueError("é…ç½®ä¸­ç¼ºå°‘ 'runner_function'ã€‚")

        runner_func = None
        if callable(runner_func_spec):
            runner_func = runner_func_spec
        elif isinstance(runner_func_spec, str): # å¦‚æœå­˜å‚¨çš„æ˜¯å‡½æ•°åå­—ç¬¦ä¸²
             if hasattr(ps, runner_func_spec):
                 runner_func = getattr(ps, runner_func_spec)
             else:
                 raise ValueError(f"è¿è¡Œå™¨å‡½æ•° '{runner_func_spec}' åœ¨ path_simulators ä¸­æœªæ‰¾åˆ°ã€‚")
        else: raise TypeError("'runner_function' å¿…é¡»æ˜¯å¯è°ƒç”¨å¯¹è±¡æˆ–å‡½æ•°åå­—ç¬¦ä¸²ã€‚")

        # --- 3. è¿è¡Œç”Ÿæˆ ---
        # è¿è¡Œå™¨å‡½æ•°æ¥æ”¶ æ¡ä»¶(X_test), æ‰©æ•£æ¨¡å‹å®ä¾‹, ç”Ÿæˆå‚æ•°, è®¾å¤‡
        all_paths_list = runner_func(
            conditions=X_test,               # ä¼ é€’å¤„ç†åçš„ 7D æ¡ä»¶
            diffusion=self.diffusion_model,  # ä¼ é€’åŠ è½½çš„ GaussianDiffusion1D å®ä¾‹
            gen_params=self.spec['generation_params'],
            device=self.device
        ) # è¿”å›æ¯ä¸ªæ¡ä»¶å¯¹åº”çš„ numpy æ•°ç»„åˆ—è¡¨

        # --- 4. æ•´åˆå¹¶ä¿å­˜ ---
        if not all_paths_list:
             print("âš ï¸ ç”Ÿæˆå™¨è¿”å›äº†ç©ºçš„è·¯å¾„åˆ—è¡¨ã€‚æ— æ–‡ä»¶ä¿å­˜ã€‚")
             return

        try:
            # æ²¿æ–°çš„ç»´åº¦ (æ¡ä»¶ç»´åº¦, axis=0) å †å åˆ—è¡¨ä¸­çš„æ•°ç»„
            final_paths_array = np.stack(all_paths_list, axis=0)
            # é¢„æœŸçš„å½¢çŠ¶åº”è¯¥æ˜¯ [num_conditions, num_paths, channels, seq_len]
            # ä¾‹å¦‚ [827, 4096, 1, 252]
            self._save_results(final_paths_array)
        except ValueError as e:
             print(f"âŒ åˆå¹¶ç”Ÿæˆçš„è·¯å¾„æ—¶å‡ºé”™: {e}ã€‚è¯·æ£€æŸ¥æ¯ä¸ªæ¡ä»¶çš„ç”Ÿæˆæ•°ç»„å½¢çŠ¶æ˜¯å¦ä¸€è‡´ã€‚")
             # å¯ä»¥å°è¯•ä¿å­˜ä¸ºä¸€ä¸ª .npz æ–‡ä»¶æˆ–å…¶ä»–æ ¼å¼ä½œä¸ºå›é€€
             # fallback_path = (self.report_dir / f"{self.job_name_safe}_generated_paths.npz").with_suffix('.npz')
             # np.savez_compressed(fallback_path, *all_paths_list)
             # print(f"   âš ï¸ å·²å°†è·¯å¾„ä½œä¸ºå•ç‹¬æ•°ç»„ä¿å­˜åˆ° .npz æ–‡ä»¶: {fallback_path}")
             raise RuntimeError("åˆå¹¶ç”Ÿæˆçš„æ‰©æ•£è·¯å¾„å¤±è´¥ã€‚") from e
    # ---


    # --- !! å·²ä¿®æ”¹ï¼šä¿å­˜ç»“æœé€»è¾‘ !! ---
    def _save_results(self, paths_array, **extra_arrays):
        """é€šç”¨ä¿å­˜é€»è¾‘ - ä½¿ç”¨ Path_Generator_Results_DIR å¹¶åŠ¨æ€ç”Ÿæˆæ–‡ä»¶å"""
        print("\nğŸ’¾ æ­£åœ¨ä¿å­˜ç»“æœ...")

        # 1. ç¡®å®šè¾“å‡ºç›®å½•
        output_dir_key = self.spec.get('output_dir', 'Path_Generator_Results_DIR')
        base_output_dir = getattr(pp, output_dir_key, None)
        if base_output_dir is None: raise AttributeError(f"PP ç¼ºå°‘ '{output_dir_key}'")
        output_dir = base_output_dir / self.asset_name
        output_dir.mkdir(parents=True, exist_ok=True)

        # --- 2. åŠ¨æ€æ„å»ºä¸»æ–‡ä»¶å ---
        filename_base = self.spec.get('output_filename_base', f"{self.job_type}_generated") # ä»é…ç½®è·å–åŸºç¡€åæˆ–é»˜è®¤
        # ä½¿ç”¨å®é™…ç”Ÿæˆçš„è·¯å¾„æ•° (self.actual_num_paths)
        output_filename = f"{filename_base}_{self.actual_num_paths}_samples.npy"
        output_path = output_dir / output_filename
        # ---

        try:
            np.save(output_path, paths_array)
            print(f"   âœ… ä¸»è·¯å¾„æ–‡ä»¶ä¿å­˜æˆåŠŸ (å½¢çŠ¶: {paths_array.shape})")
            print(f"      -> {output_path}")
            try: print(f"      -> æ–‡ä»¶å¤§å°: {output_path.stat().st_size / 1024**2:.2f} MB")
            except OSError: pass
        except Exception as e: print(f"   âŒ ä¿å­˜ä¸»è·¯å¾„æ–‡ä»¶æ—¶å‡ºé”™: {e}")

        # 3. ä¿å­˜é¢å¤–æ–‡ä»¶ (æ–‡ä»¶åä¹ŸåŠ¨æ€ç”Ÿæˆ)
        if extra_arrays:
            # ä½¿ç”¨ä¸å«æ•°é‡çš„åŸºç¡€åæ¥æ„å»ºé¢å¤–æ–‡ä»¶å
            extra_base = f"{filename_base}"
            if self.job_type == 'mc' and 'garch' in self.job_name.lower():
                 extra_base += "_fitted" # ä¿æŒ GARCH æ–‡ä»¶åä¸€è‡´æ€§

            for key, array_data in extra_arrays.items():
                extra_filename = f"{extra_base}_{key}_{self.actual_num_paths}_samples.npy" # åŠ å…¥æ•°é‡
                extra_output_path = output_dir / extra_filename
                try:
                    np.save(extra_output_path, array_data)
                    print(f"   âœ… é¢å¤–æ–‡ä»¶ '{key}' ä¿å­˜æˆåŠŸ (å½¢çŠ¶: {array_data.shape})")
                    print(f"      -> {extra_output_path}")
                except Exception as e: print(f"   âŒ ä¿å­˜é¢å¤–æ–‡ä»¶ '{key}' æ—¶å‡ºé”™: {e}")
        # 4. æ¸…ç†å†…å­˜
        del paths_array, extra_arrays; gc.collect()
        if self.device.startswith('cuda'):
            if torch.cuda.is_available() and torch.cuda.is_initialized():
                 print(f"   GPU æ˜¾å­˜å ç”¨: {torch.cuda.memory_allocated() / 1024**2:.1f} MB")
                 torch.cuda.empty_cache()
                 print(f"   GPU æ˜¾å­˜ (ç¼“å­˜æ¸…ç†å): {torch.cuda.memory_allocated() / 1024**2:.1f} MB")
            elif self.device == 'cuda': print(f"   è®¾å¤‡æ˜¯ CUDA ä½† torch.cuda ä¸å¯ç”¨/æœªåˆå§‹åŒ–ã€‚")
    # ---